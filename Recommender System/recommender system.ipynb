{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this post I am going to talk about how to apply the natural lanugage processing techniques to the recommender system. In many real cases, for instance, if we have finished a course in Coursera, we want to learn some related courses to get a deeper insight into one research field. Hence, it would be better if our online system could automatically recommend some relevant courses for a learner.\n",
    "\n",
    "If we talk more about this coursera case, some people might say we could use labels to tag all the courses previously and if one learner finished one course, based on the label the system would post some relevant courses. However, manually labelling huge number of courses is very time consuming. Moreover, some algorithms such as collaborative filtering and content-based filtering could also help this problem. But these approaches would not work if we had a new course on Coursera and we did not have much feedback(data) from users about this course. **A better approach** to this problem is that we utilize the title and the description of each course and use NLP techniques to work out for instance the similarity of two courses. Based on this similarity, our system could automatically recommende coursers to our users. The following sections will show the main techniques we use.\n",
    "\n",
    "So let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construct the NLP Models\n",
    "\n",
    "In this section we show how to create a recommendation system for our Coursera users. Let's first import modules for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load the dataset\n",
    "\n",
    "First we import all the modules we need in this case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the data. The data can be found in here: [Coursera corpus](https://github.com/bright1993ff66/Text-Data-Analysis/blob/master/Recommender%20System/coursera_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Writing II: Rhetorical Composing\\tRhetorical Composing engages you in a series of interactive reading, research, and composing activities along with assignments designed to help you become more effective consumers and producers of alphabetic, visual and multimodal texts.  Join us to become more effective writers... and better citizens.\\tRhetorical Composing is a course where writers exchange words, ideas,     talents, and support. You will be introduced to a variety of rhetorical     concepts—that is, ideas and techniques to inform and persuade audiences—that     will help you become a more effective consumer and producer of written,     visual, and multimodal texts. The class includes short videos, demonstrations,     and activities. We envision Rhetorical Composing as a learning community that includes both those enrolled in this course and the instructors. We bring our expertise in writing, rhetoric and course design, and we have designed the assignments and course infrastructure to help you share your experiences as writers, students, and professionals with each other and with us. These collaborations are facilitated through WEx, The Writers Exchange, a place where you will exchange your work and feedback',\n",
       " 'Genetics and Society: A Course for Educators\\tHow have advances in genetics affected society? What do we need to know to make ethical decisions about genetic technologies? This course includes the study of cloning, genetic enhancement, and ownership of genetic information. Course participants will acquire the tools to explore the ethics of modern genetics and learn how to integrate these issues into their classrooms.\\tThe AMNH course Genetics and Society: A Course for Educators explores the social, legal and ethical issues of modern-day genetics. Informed by the recently released Next Generation Science Standards, the course provides an overview of recent genetic discoveries and molecular lab techniques. Participants will acquire an understanding of the science and technology behind breakthroughs such as therapeutic cloning and the sequencing of the human genome. You will also have the opportunity to discuss and debate issues surrounding hot-button topics in genetics: If it is ethical to clone your dog, will humans be next? Should we care if our food is genetically modified? What are the pros and cons of gene therapy?&nbsp;Course participants will bring their understanding of modern genetics and associated ethical issues - along with content resources, discussion questions, and assignments - into their own teaching.&nbsp;']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('F:/Data Analysis/github/Text-Data-Analysis/Recommender System/coursera_corpus', 'r', encoding = 'UTF-8')\n",
    "courses = [line.strip() for line in file]\n",
    "courses[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we see that the names of the courses are seperated by tab. Hence we use the following code to extract the courses' names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Writing II: Rhetorical Composing',\n",
       " 'Genetics and Society: A Course for Educators',\n",
       " 'General Game Playing',\n",
       " 'Genes and the Human Condition (From Behavior to Biotechnology)',\n",
       " 'A Brief History of Humankind',\n",
       " 'New Models of Business in Society',\n",
       " 'Analyse Numérique pour Ingénieurs',\n",
       " 'Evolution: A Course for Educators',\n",
       " 'Coding the Matrix: Linear Algebra through Computer Science Applications',\n",
       " 'The Dynamic Earth: A Course for Educators']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_name = [course.split('\\t')[0] for course in courses]\n",
    "courses_name[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2 Preprocess the data\n",
    "\n",
    "Generally, to improve the quality of our text data, we always consider the following steps:\n",
    "\n",
    "1. lowercase the words\n",
    "2. eliminate punctuations\n",
    "3. eliminate the stopwords\n",
    "3. stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['writing', 'ii:', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'you', 'in', 'a', 'series', 'of', 'interactive', 'reading,', 'research,', 'and', 'composing', 'activities', 'along', 'with', 'assignments', 'designed', 'to', 'help', 'you', 'become', 'more', 'effective', 'consumers', 'and', 'producers', 'of', 'alphabetic,', 'visual', 'and', 'multimodal', 'texts.', 'join', 'us', 'to', 'become', 'more', 'effective', 'writers...', 'and', 'better', 'citizens.', 'rhetorical', 'composing', 'is', 'a', 'course', 'where', 'writers', 'exchange', 'words,', 'ideas,', 'talents,', 'and', 'support.', 'you', 'will', 'be', 'introduced', 'to', 'a', 'variety', 'of', 'rhetorical', 'concepts—that', 'is,', 'ideas', 'and', 'techniques', 'to', 'inform', 'and', 'persuade', 'audiences—that', 'will', 'help', 'you', 'become', 'a', 'more', 'effective', 'consumer', 'and', 'producer', 'of', 'written,', 'visual,', 'and', 'multimodal', 'texts.', 'the', 'class', 'includes', 'short', 'videos,', 'demonstrations,', 'and', 'activities.', 'we', 'envision', 'rhetorical', 'composing', 'as', 'a', 'learning', 'community', 'that', 'includes', 'both', 'those', 'enrolled', 'in', 'this', 'course', 'and', 'the', 'instructors.', 'we', 'bring', 'our', 'expertise', 'in', 'writing,', 'rhetoric', 'and', 'course', 'design,', 'and', 'we', 'have', 'designed', 'the', 'assignments', 'and', 'course', 'infrastructure', 'to', 'help', 'you', 'share', 'your', 'experiences', 'as', 'writers,', 'students,', 'and', 'professionals', 'with', 'each', 'other', 'and', 'with', 'us.', 'these', 'collaborations', 'are', 'facilitated', 'through', 'wex,', 'the', 'writers', 'exchange,', 'a', 'place', 'where', 'you', 'will', 'exchange', 'your', 'work', 'and', 'feedback']\t"
     ]
    }
   ],
   "source": [
    "text_lower = [[word for word in document.lower().split()] for document in courses]\n",
    "print(text_lower[0], end = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the text above, we see some output such as 'texts.' does not split the word and the punctuation. Hence, it would be better if we could use the **word_tokenize** function in nltk and get a more meaningful result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['writing', 'ii', ':', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'you', 'in', 'a', 'series', 'of', 'interactive', 'reading', ',', 'research', ',', 'and', 'composing', 'activities', 'along', 'with', 'assignments', 'designed', 'to', 'help', 'you', 'become', 'more', 'effective', 'consumers', 'and', 'producers', 'of', 'alphabetic', ',', 'visual', 'and', 'multimodal', 'texts', '.', 'join', 'us', 'to', 'become', 'more', 'effective', 'writers', '...', 'and', 'better', 'citizens', '.', 'rhetorical', 'composing', 'is', 'a', 'course', 'where', 'writers', 'exchange', 'words', ',', 'ideas', ',', 'talents', ',', 'and', 'support', '.', 'you', 'will', 'be', 'introduced', 'to', 'a', 'variety', 'of', 'rhetorical', 'concepts—that', 'is', ',', 'ideas', 'and', 'techniques', 'to', 'inform', 'and', 'persuade', 'audiences—that', 'will', 'help', 'you', 'become', 'a', 'more', 'effective', 'consumer', 'and', 'producer', 'of', 'written', ',', 'visual', ',', 'and', 'multimodal', 'texts', '.', 'the', 'class', 'includes', 'short', 'videos', ',', 'demonstrations', ',', 'and', 'activities', '.', 'we', 'envision', 'rhetorical', 'composing', 'as', 'a', 'learning', 'community', 'that', 'includes', 'both', 'those', 'enrolled', 'in', 'this', 'course', 'and', 'the', 'instructors', '.', 'we', 'bring', 'our', 'expertise', 'in', 'writing', ',', 'rhetoric', 'and', 'course', 'design', ',', 'and', 'we', 'have', 'designed', 'the', 'assignments', 'and', 'course', 'infrastructure', 'to', 'help', 'you', 'share', 'your', 'experiences', 'as', 'writers', ',', 'students', ',', 'and', 'professionals', 'with', 'each', 'other', 'and', 'with', 'us', '.', 'these', 'collaborations', 'are', 'facilitated', 'through', 'wex', ',', 'the', 'writers', 'exchange', ',', 'a', 'place', 'where', 'you', 'will', 'exchange', 'your', 'work', 'and', 'feedback']\t"
     ]
    }
   ],
   "source": [
    "text_lower = [[word.lower() for word in word_tokenize(document)] for document in courses]\n",
    "print(text_lower[0], end = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words and the punctuations have been splitted. Then we need to do is eliminate the punctuatiaons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['writing', 'ii', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'you', 'in', 'a', 'series', 'of', 'interactive', 'reading', 'research', 'and', 'composing', 'activities', 'along', 'with', 'assignments', 'designed', 'to', 'help', 'you', 'become', 'more', 'effective', 'consumers', 'and', 'producers', 'of', 'alphabetic', 'visual', 'and', 'multimodal', 'texts', 'join', 'us', 'to', 'become', 'more', 'effective', 'writers', 'and', 'better', 'citizens', 'rhetorical', 'composing', 'is', 'a', 'course', 'where', 'writers', 'exchange', 'words', 'ideas', 'talents', 'and', 'support', 'you', 'will', 'be', 'introduced', 'to', 'a', 'variety', 'of', 'rhetorical', 'concepts—that', 'is', 'ideas', 'and', 'techniques', 'to', 'inform', 'and', 'persuade', 'audiences—that', 'will', 'help', 'you', 'become', 'a', 'more', 'effective', 'consumer', 'and', 'producer', 'of', 'written', 'visual', 'and', 'multimodal', 'texts', 'the', 'class', 'includes', 'short', 'videos', 'demonstrations', 'and', 'activities', 'we', 'envision', 'rhetorical', 'composing', 'as', 'a', 'learning', 'community', 'that', 'includes', 'both', 'those', 'enrolled', 'in', 'this', 'course', 'and', 'the', 'instructors', 'we', 'bring', 'our', 'expertise', 'in', 'writing', 'rhetoric', 'and', 'course', 'design', 'and', 'we', 'have', 'designed', 'the', 'assignments', 'and', 'course', 'infrastructure', 'to', 'help', 'you', 'share', 'your', 'experiences', 'as', 'writers', 'students', 'and', 'professionals', 'with', 'each', 'other', 'and', 'with', 'us', 'these', 'collaborations', 'are', 'facilitated', 'through', 'wex', 'the', 'writers', 'exchange', 'a', 'place', 'where', 'you', 'will', 'exchange', 'your', 'work', 'and', 'feedback']\t"
     ]
    }
   ],
   "source": [
    "x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "text_lower_no_punctuation = []\n",
    "\n",
    "for text in text_lower:\n",
    "    new_text = []\n",
    "    for token in text:\n",
    "        new_token = x.sub(u'',token) \n",
    "        if not new_token == u'':\n",
    "            new_text.append(new_token)\n",
    "    text_lower_no_punctuation.append(new_text)\n",
    "    \n",
    "print(text_lower_no_punctuation[0], end = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The punctuations have been removed. After that, we need to remove the unmeaningful stopwords. Fortunately, nltk provides us with a list of stopwords which are helpful in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['writing', 'ii', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'series', 'interactive', 'reading', 'research', 'composing', 'activities', 'along', 'assignments', 'designed', 'help', 'become', 'effective', 'consumers', 'producers', 'alphabetic', 'visual', 'multimodal', 'texts', 'join', 'us', 'become', 'effective', 'writers', 'better', 'citizens', 'rhetorical', 'composing', 'course', 'writers', 'exchange', 'words', 'ideas', 'talents', 'support', 'introduced', 'variety', 'rhetorical', 'concepts—that', 'ideas', 'techniques', 'inform', 'persuade', 'audiences—that', 'help', 'become', 'effective', 'consumer', 'producer', 'written', 'visual', 'multimodal', 'texts', 'class', 'includes', 'short', 'videos', 'demonstrations', 'activities', 'envision', 'rhetorical', 'composing', 'learning', 'community', 'includes', 'enrolled', 'course', 'instructors', 'bring', 'expertise', 'writing', 'rhetoric', 'course', 'design', 'designed', 'assignments', 'course', 'infrastructure', 'help', 'share', 'experiences', 'writers', 'students', 'professionals', 'us', 'collaborations', 'facilitated', 'wex', 'writers', 'exchange', 'place', 'exchange', 'work', 'feedback'], ['genetics', 'society', 'course', 'educators', 'advances', 'genetics', 'affected', 'society', 'need', 'know', 'make', 'ethical', 'decisions', 'genetic', 'technologies', 'course', 'includes', 'study', 'cloning', 'genetic', 'enhancement', 'ownership', 'genetic', 'information', 'course', 'participants', 'acquire', 'tools', 'explore', 'ethics', 'modern', 'genetics', 'learn', 'integrate', 'issues', 'classrooms', 'amnh', 'course', 'genetics', 'society', 'course', 'educators', 'explores', 'social', 'legal', 'ethical', 'issues', 'modernday', 'genetics', 'informed', 'recently', 'released', 'next', 'generation', 'science', 'standards', 'course', 'provides', 'overview', 'recent', 'genetic', 'discoveries', 'molecular', 'lab', 'techniques', 'participants', 'acquire', 'understanding', 'science', 'technology', 'behind', 'breakthroughs', 'therapeutic', 'cloning', 'sequencing', 'human', 'genome', 'also', 'opportunity', 'discuss', 'debate', 'issues', 'surrounding', 'hotbutton', 'topics', 'genetics', 'ethical', 'clone', 'dog', 'humans', 'next', 'care', 'food', 'genetically', 'modified', 'pros', 'cons', 'gene', 'therapy', 'nbsp', 'course', 'participants', 'bring', 'understanding', 'modern', 'genetics', 'associated', 'ethical', 'issues', 'along', 'content', 'resources', 'discussion', 'questions', 'assignments', 'teaching', 'nbsp']]\t"
     ]
    }
   ],
   "source": [
    "text_filtered = [[word for word in document if word not in english_stopwords ] for document in text_lower_no_punctuation]\n",
    "print(text_filtered[0:2], end = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we do the stemming for this text. Stemming in NLP means that we treat each word's different variants as the same word. For instance for playing, played, play, we see them as a same word:play. nltk has many useful stemmers. The most well-know ones are Lancaster Stemmer and Porter Stemmer. Here we use Lancaster Stemmer to cope with this Coursera corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['writ', 'ii', 'rhet', 'compos', 'rhet', 'compos', 'eng', 'sery', 'interact', 'read', 'research', 'compos', 'act', 'along', 'assign', 'design', 'help', 'becom', 'effect', 'consum', 'produc', 'alphabet', 'vis', 'multimod', 'text', 'join', 'us', 'becom', 'effect', 'writ', 'bet', 'cit', 'rhet', 'compos', 'cours', 'writ', 'exchang', 'word', 'idea', 'tal', 'support', 'introduc', 'vary', 'rhet', 'concepts—that', 'idea', 'techn', 'inform', 'persuad', 'audiences—that', 'help', 'becom', 'effect', 'consum', 'produc', 'writ', 'vis', 'multimod', 'text', 'class', 'includ', 'short', 'video', 'demonst', 'act', 'envid', 'rhet', 'compos', 'learn', 'commun', 'includ', 'enrol', 'cours', 'instruct', 'bring', 'expert', 'writ', 'rhet', 'cours', 'design', 'design', 'assign', 'cours', 'infrastruct', 'help', 'shar', 'expery', 'writ', 'stud', 'profess', 'us', 'collab', 'facilit', 'wex', 'writ', 'exchang', 'plac', 'exchang', 'work', 'feedback']\t"
     ]
    }
   ],
   "source": [
    "st = LancasterStemmer()\n",
    "text_stemmed = [[st.stem(word) for word in docment] for docment in text_filtered]\n",
    "print(text_stemmed[0], end = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Text Data Analysis with Various Models\n",
    "\n",
    "We have finished the text data preprocessing. Next, we will fead the data to our NLP models. To calculate the similarity between two courses, one possible approach is to use topic models such as LDA and LSI to work out the inner relationship between these courses.\n",
    "\n",
    "Hence let's start with the LSI model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 LSI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct the bag-of-words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5886 unique tokens: ['’', 'dieu', 'singl', 'spring', 'millenn']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dictionary, we get the id of each unique word. Then we could calculate how many times a word appears in this Coursera corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "doc2bow converts `document` (a list of words) into the bag-of-words format\n",
    "'''\n",
    "corpus = [dictionary.doc2bow(text) for text in text_stemmed] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 2), (8, 3), (9, 3), (10, 1), (11, 1), (12, 1), (13, 2), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (19, 7), (20, 6), (21, 1), (22, 2), (23, 4), (24, 1), (25, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 5), (32, 1), (33, 1), (34, 1), (35, 2), (36, 1), (37, 1), (38, 2), (39, 1), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 3), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 3), (52, 1), (53, 3), (54, 2), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1)] "
     ]
    }
   ],
   "source": [
    "print(corpus[0], end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to figure out the importance of a word, intuitively, we consider using the TF-IDF value. The value of TF-IDF could be calculated as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "TF-IDF(x) = TF(X) \\times IDF(X)\n",
    "\\end{equation}\n",
    "\n",
    "where $TF(x)$ means the word frequency of a word $x$. $IDF(x)$ represents the importance of a word $X$. We usually use the following formular to calcualte the $IDF(X)$:\n",
    "\n",
    "\\begin{equation}\n",
    "IDF(X) = log\\frac{N+1}{N(x)+1}\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ means the number of documents and $N(x)$ means the number of documents with word $x$. Sometimes, we could not find the term $x$ in all of our documents. Hence, it would be better if we could just add 1 to both the numerator and the denominator.\n",
    "\n",
    "Hence, we could train a TF-IDF model based on our bag-of-words format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could calculate the TF-IDF value for each word in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06942854357795676), (1, 0.04848606348417998), (2, 0.05549012404561855), (3, 0.08227414712216302), (4, 0.06494137473217033), (5, 0.06494137473217033), (6, 0.11939725492070859), (7, 0.07911819315429061), (8, 0.12990421983608894), (9, 0.22564053109367346), (10, 0.07758199632369395), (11, 0.11939725492070859), (12, 0.11939725492070859), (13, 0.23879450984141717), (14, 0.12037571819073922), (15, 0.033679499551263856), (16, 0.03640516876863603), (17, 0.11939725492070859), (18, 0.04848606348417998), (19, 0.3096581070133701), (20, 0.5838322958557992), (21, 0.01837260049835496), (22, 0.1093384781995657), (23, 0.009926203812253558), (24, 0.043763700276130434), (25, 0.13265748941711367), (26, 0.11939725492070859), (27, 0.046249439563031396), (28, 0.08336696311029498), (29, 0.011399692365667558), (30, 0.02451348611154219), (31, 0.38790998161846973), (32, 0.03484555886352626), (33, 0.04848606348417998), (34, 0.10545883538837036), (35, 0.1093384781995657), (36, 0.06942854357795676), (37, 0.04240753106432324), (38, 0.0610785615827926), (39, 0.049705157259017516), (40, 0.057239826144367865), (41, 0.022058022218786385), (42, 0.07758199632369395), (43, 0.01709134627606883), (44, 0.06942854357795676), (45, 0.0880889612390743), (46, 0.049705157259017516), (47, 0.031029765808407047), (48, 0.06942854357795676), (49, 0.0354549657646678), (50, 0.04033261073844281), (51, 0.1092155063059081), (52, 0.05100295519983211), (53, 0.08740729674093296), (54, 0.04241906866790852), (55, 0.03608341968474428), (56, 0.0730948274779075), (57, 0.05100295519983211), (58, 0.05549012404561855), (59, 0.030783027881683914), (60, 0.05817529196281967), (61, 0.06364357679135574)] "
     ]
    }
   ],
   "source": [
    "docs = [doc for doc in corpus_tfidf]\n",
    "print(docs[0], end =' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use **Latent Semantic Index(LSI)** or **Latent Semantic Analysis(LSA)** model to train our data. The LSI model assumes that **words that are close in meaning will occur in similar pieces**. The LSI model is based on singular value decomposition(SVD). If we use SVD and implement PCA with first $k$ features, for a $m \\times n$ matrix $A$, we could get：\n",
    "\n",
    "\\begin{equation}\n",
    "A_{m \\times n} \\approx U_{m \\times k} \\Sigma_{k \\times k} V_{k \\times n}\n",
    "\\end{equation}\n",
    "\n",
    "We always use $A_{m \\times n}$ to represent the feature value of mth word in the nth document. In this case, we use TF-IDF value to be this feature value. Then for the terms $ U_{m \\times k}, \\Sigma_{k \\times k}$ and $V_{k \\times n} $:\n",
    "\n",
    "1. $U_{m \\times k}$ represents the correlation between mth word and kth topic (an orthogonal matrix)\n",
    "2. $\\Sigma_{k \\times k}$ represents the correlatiion between the topics (a diagonal matrix)\n",
    "3. $V_{k \\times n} $ represents the correlation between the kth topic and the nth document (an orthogonal matrix)\n",
    "\n",
    "So now an important question is: how to choose an appropriate k? This number is a hyperparameter to be carefully selected. In general, we could work out an optimum k by selecting the N largest singular values of the TF-IDF corpus matrix. In gensim, we could use a function called **corpus2dense** to calculate the singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy_matrix = matutils.corpus2dense(corpus, num_terms = 31730) # num_terms means number of words in text_stemmed\n",
    "s = np.linalg.svd(numpy_matrix, full_matrices=False, compute_uv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAFDCAYAAAAef4vuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/H/rWWheSMdKGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a57e0b2780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(s, bins=100)\n",
    "plt.xlabel('Singular values', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the plot above, we see that most singular values are between 0 and 30. Then we could try to draw the following plot and see if we could work out the number of topics by seeing the proportion of sum of first k diagonal values of s over the sum of all the diagonal values of s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a57e358ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = []\n",
    "\n",
    "for i in range(len(courses_name)+1):\n",
    "    values.append(sum(list(s)[:i])/sum(list(s)))\n",
    "values.pop(0)\n",
    "    \n",
    "plt.plot(list(range(len(courses_name))), values)\n",
    "plt.xlabel('Course Number')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the firgure above, we see the the proportion grows steadily and it seems hard to find a very appropriate number of topics. But let's try k = 10 first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the function models.LsiModel, we use the TF-IDF values as the inputs and we assume that there are 10 different topics for this Coursera dataset. Then we could construct the following LSI model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus = corpus_tfidf, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see each topic by using the print_topic function. From the results we could see some negative values, which is the main reason why the result given by LSA is hard to interprete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.254*\"teach\" + 0.152*\"nbsp\" + 0.136*\"program\" + 0.121*\"learn\" + 0.118*\"mus\" + 0.105*\"heal\" + 0.101*\"comput\" + 0.099*\"stud\" + 0.096*\"network\" + 0.093*\"design\"'),\n",
       " (1,\n",
       "  '0.591*\"de\" + 0.381*\"la\" + 0.216*\"en\" + 0.174*\"à\" + 0.162*\"cour\" + 0.148*\"los\" + 0.144*\"que\" + 0.141*\"les\" + 0.136*\"un\" + 0.111*\"curso\"'),\n",
       " (2,\n",
       "  '0.628*\"teach\" + 0.200*\"portfolio\" + 0.186*\"assist\" + -0.162*\"mus\" + 0.128*\"improv\" + 0.122*\"profess\" + 0.116*\"learn\" + 0.114*\"undertak\" + 0.113*\"strongly\" + -0.111*\"network\"'),\n",
       " (3,\n",
       "  '0.806*\"mus\" + 0.171*\"sound\" + -0.168*\"heal\" + 0.095*\"audio\" + 0.093*\"art\" + 0.090*\"digit\" + -0.084*\"econom\" + -0.079*\"glob\" + 0.073*\"composit\" + 0.069*\"program\"'),\n",
       " (4,\n",
       "  '0.372*\"heal\" + -0.286*\"program\" + -0.213*\"comput\" + 0.213*\"mus\" + -0.184*\"dat\" + -0.183*\"langu\" + -0.180*\"algorithm\" + 0.166*\"glob\" + -0.142*\"network\" + 0.126*\"food\"'),\n",
       " (5,\n",
       "  '0.507*\"heal\" + 0.246*\"program\" + 0.163*\"nutrit\" + -0.155*\"busy\" + -0.148*\"gam\" + -0.137*\"network\" + 0.132*\"langu\" + 0.130*\"diseas\" + -0.125*\"econom\" + 0.118*\"diet\"'),\n",
       " (6,\n",
       "  '-0.499*\"chem\" + -0.313*\"org\" + -0.299*\"react\" + 0.192*\"heal\" + -0.162*\"compound\" + -0.140*\"biolog\" + -0.138*\"structure\" + 0.137*\"network\" + 0.128*\"gam\" + 0.120*\"program\"'),\n",
       " (7,\n",
       "  '0.569*\"gam\" + -0.367*\"network\" + 0.186*\"program\" + 0.169*\"busy\" + -0.110*\"evolv\" + -0.107*\"mus\" + -0.099*\"dat\" + 0.096*\"art\" + 0.094*\"chem\" + -0.093*\"internet\"'),\n",
       " (8,\n",
       "  '-0.265*\"network\" + 0.258*\"evolv\" + 0.180*\"ear\" + 0.175*\"hist\" + 0.174*\"genet\" + -0.170*\"busy\" + -0.143*\"fin\" + 0.141*\"genom\" + 0.139*\"gam\" + -0.137*\"econom\"'),\n",
       " (9,\n",
       "  '0.450*\"network\" + 0.448*\"gam\" + -0.178*\"dat\" + -0.169*\"fin\" + -0.144*\"stat\" + -0.140*\"busy\" + 0.140*\"heal\" + 0.114*\"soc\" + 0.110*\"internet\" + 0.110*\"chem\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this LSI model, we could project our corpus to a 10-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could compute the similarity between different documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if a user has finished Andrew Ng's course _Machine learning_, he wants to get a deeper insight into machine learning such as neural networks. Hence, firstly we construct the bag-of-words framework for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_course = text_stemmed[210]\n",
    "ml_bow = dictionary.doc2bow(ml_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then based on our previous LSI model, we could work out the representation of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 8.5473598416545293), (1, -0.65678271417205381), (2, 0.97280235465731391), (3, 0.14003119477396811), (4, -4.5406659363138191), (5, 0.78353223837662755), (6, 1.7710098487537833), (7, -1.6304250302501397), (8, -0.10531251667061418), (9, 0.80833432240431891)]\n"
     ]
    }
   ],
   "source": [
    "ml_lsi = lsi[ml_bow]\n",
    "print(ml_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use this to locate one row in the similarity matrix and work out the top 10 most similar courses to machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims = index[ml_lsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(210, 1.0), (174, 0.97745174), (189, 0.95145911), (141, 0.94276065), (63, 0.94158185), (238, 0.94119596), (184, 0.9388023), (220, 0.93320107), (219, 0.92692423), (221, 0.9248457)]\n"
     ]
    }
   ],
   "source": [
    "sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sort_sims[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we finally get the courses which are most close to Andrew's course 'machine learning' are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine Learning', 'Machine Learning', 'Computer Security', 'Computational Photography', 'Human-Computer Interaction', 'Probabilistic Graphical Models', 'Computer Science 101', 'Algorithms: Design and Analysis, Part 2', 'Cryptography II', 'Heterogeneous Parallel Programming']\n"
     ]
    }
   ],
   "source": [
    "course_id = [course[0] for course in sort_sims[:10]]\n",
    "top_10_courses = [courses_name[i] for i in course_id]\n",
    "print(top_10_courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results look quite reasonable. But the LSI model does have the following **issues**:\n",
    "\n",
    "1. This model is computationally expensive, especially when we are analyzing the large corpus(The speed could be improved by implementing the NMF decomposition).\n",
    "2. It is hard to select the appropriate $k$.\n",
    "3. The result is hard to interpret.\n",
    "\n",
    "4. The LSI model does not take into account **the prior information**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3.2 LDA Model\n",
    "\n",
    "In this section, we use another topic model called **Latent Dirichlet allocation(LDA)** to do the recommendation.\n",
    "\n",
    "For this Coursera corpus, we could also run a LDA model using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus_tfidf, id2word = dictionary, num_topics = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike LSA, the topics coming from LDA are easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"teach\" + 0.002*\"learn\" + 0.001*\"nbsp\" + 0.001*\"design\" + 0.001*\"im\" + 0.001*\"pract\" + 0.001*\"heal\" + 0.001*\"comput\" + 0.001*\"educ\" + 0.001*\"techn\"'),\n",
       " (1,\n",
       "  '0.003*\"heal\" + 0.003*\"mus\" + 0.002*\"law\" + 0.002*\"teach\" + 0.001*\"publ\" + 0.001*\"nbsp\" + 0.001*\"design\" + 0.001*\"rel\" + 0.001*\"stud\" + 0.001*\"network\"'),\n",
       " (2,\n",
       "  '0.001*\"teach\" + 0.001*\"onlin\" + 0.001*\"langu\" + 0.001*\"vaccin\" + 0.001*\"sci\" + 0.001*\"program\" + 0.001*\"dat\" + 0.001*\"archaeolog\" + 0.001*\"tink\" + 0.001*\"philosoph\"'),\n",
       " (3,\n",
       "  '0.002*\"nbsp\" + 0.002*\"heal\" + 0.001*\"model\" + 0.001*\"teach\" + 0.001*\"design\" + 0.001*\"gam\" + 0.001*\"mus\" + 0.001*\"research\" + 0.001*\"process\" + 0.001*\"comput\"'),\n",
       " (4,\n",
       "  '0.002*\"gam\" + 0.002*\"de\" + 0.002*\"react\" + 0.001*\"program\" + 0.001*\"writ\" + 0.001*\"chem\" + 0.001*\"contraceiv\" + 0.001*\"epigenet\" + 0.001*\"org\" + 0.001*\"robot\"'),\n",
       " (5,\n",
       "  '0.001*\"engin\" + 0.001*\"busy\" + 0.001*\"teach\" + 0.001*\"heal\" + 0.001*\"analys\" + 0.001*\"aid\" + 0.001*\"learn\" + 0.001*\"innov\" + 0.001*\"econom\" + 0.001*\"car\"'),\n",
       " (6,\n",
       "  '0.002*\"food\" + 0.002*\"evolv\" + 0.002*\"genet\" + 0.002*\"dat\" + 0.002*\"system\" + 0.002*\"program\" + 0.001*\"planet\" + 0.001*\"univers\" + 0.001*\"nbsp\" + 0.001*\"im\"'),\n",
       " (7,\n",
       "  '0.001*\"de\" + 0.001*\"teach\" + 0.001*\"mus\" + 0.001*\"la\" + 0.001*\"vary\" + 0.001*\"feedback\" + 0.001*\"algebr\" + 0.001*\"network\" + 0.001*\"stud\" + 0.001*\"writ\"'),\n",
       " (8,\n",
       "  '0.002*\"program\" + 0.002*\"nbsp\" + 0.002*\"teach\" + 0.002*\"de\" + 0.002*\"grow\" + 0.002*\"langu\" + 0.001*\"busy\" + 0.001*\"hist\" + 0.001*\"mus\" + 0.001*\"sec\"'),\n",
       " (9,\n",
       "  '0.001*\"econom\" + 0.001*\"person\" + 0.001*\"risk\" + 0.001*\"network\" + 0.001*\"behavy\" + 0.001*\"cre\" + 0.001*\"de\" + 0.001*\"latin\" + 0.001*\"art\" + 0.001*\"optim\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, a trained model could be used to transform documents into LDA topic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_lda = lda[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lda[corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's try the Andrew's machine learning course again and see the top 10 courses recommended by our LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.88237036584137329), (8, 0.11305706453038829)]\n"
     ]
    }
   ],
   "source": [
    "ml_lda = lda[ml_bow]\n",
    "print(ml_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims = index[ml_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(210, 1.0), (269, 0.99999052), (220, 0.99994886), (15, 0.9996441), (130, 0.99921817), (183, 0.99903971), (268, 0.99870664), (89, 0.99843609), (287, 0.99835515), (299, 0.99800199)]\n"
     ]
    }
   ],
   "source": [
    "sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sort_sims[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine Learning', 'Foundations of Teaching for Learning 1: Introduction', 'Algorithms: Design and Analysis, Part 2', 'The Ancient Greeks', 'Growing Old Around the Globe', 'Model Thinking', 'Unpredictable? Randomness, Chance and Free Will', 'Introduction to Computational Finance and Financial Econometrics', 'Women and the Civil Rights Movement', 'Sports and Society']\n"
     ]
    }
   ],
   "source": [
    "course_id = [course[0] for course in sort_sims[:10]]\n",
    "top_10_courses = [courses_name[i] for i in course_id]\n",
    "print(top_10_courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, it seems that courses recommended by our LDA model is not reasonable. Courses such as 'The Ancient Greeks' seems to have little correlation with machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is currently one of the most successful unsupervised learning methods. It is extremely user-friendly and it could work out the semantic correlation between words. For this Coursera case, we could also use the word representations to work out the similarity between different documents.\n",
    "\n",
    "Firstly, let's use models in gensim to work out the word vectors(here we use the **text_filtered** to train the word representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_model = models.Word2Vec(text_filtered, size = 100, window = 5, min_count = 0, workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save the model and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_model.save('word2vec')\n",
    "wv_model = models.Word2Vec.load('word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could view the word vector of some words using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13106894, -0.12167535,  0.24640566, -0.05197863,  0.2219456 ,\n",
       "       -0.00565952, -0.11794956,  0.21845642,  0.11507841, -0.01180338,\n",
       "        0.33872506,  0.31828687, -0.00413977,  0.1855204 , -0.24783   ,\n",
       "       -0.07101133, -0.412938  , -0.40357032, -0.17756958,  0.238086  ,\n",
       "       -0.0826776 , -0.24603625, -0.05968533, -0.13996242,  0.1237243 ,\n",
       "       -0.01276028, -0.16900101,  0.00170963, -0.16168624,  0.10071407,\n",
       "       -0.14015746, -0.14182845, -0.31377584,  0.11196701, -0.14182384,\n",
       "        0.14483951, -0.17727917,  0.09155634, -0.05148064,  0.06514022,\n",
       "       -0.10216264, -0.01642863, -0.31678283,  0.30057669,  0.2147166 ,\n",
       "       -0.38663855, -0.15629113,  0.19969514,  0.02738849,  0.04776668,\n",
       "        0.51250279, -0.06700592,  0.61544526, -0.02389247,  0.26460132,\n",
       "       -0.12843534,  0.28052175, -0.21679725, -0.44115797,  0.11255834,\n",
       "        0.25031665,  0.05851799,  0.0104895 ,  0.20953378, -0.01893906,\n",
       "       -0.06174672,  0.0590329 , -0.24707048,  0.08813428,  0.03866802,\n",
       "       -0.10730993,  0.13601343,  0.01799754, -0.02010497,  0.17643742,\n",
       "       -0.06887777, -0.25858289,  0.13521014,  0.23014919, -0.0359284 ,\n",
       "       -0.07900213, -0.18422693, -0.06963287,  0.1949762 ,  0.0712783 ,\n",
       "       -0.01837075, -0.23659594, -0.06051736, -0.0118034 ,  0.13400637,\n",
       "        0.24968405,  0.08946441, -0.39862445, -0.33008724,  0.18895291,\n",
       "       -0.10419115,  0.00409971,  0.13842732,  0.04755352, -0.5720228 ], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.wv['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have got the word vector of a word. So how to get the representation of a course? A common idea is that we could use the TF-IDF value of a word as weight. If we use $D_i$ to denote the representation of a document i and let n denote the number of words in this document, $T_{i,j}$ and $W_{i,j}$ represents the TF-IDF value and word vector of a word, then the representation of a document is:\n",
    "\n",
    "\\begin{equation}\n",
    "D_i = \\sum_{j=1}^{n} T_{i,j}W_{i,j}\n",
    "\\end{equation}\n",
    "\n",
    "Firstly, we calculate the TF-IDF value of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_value(corpus, target_word):\n",
    "    count_tf = 0\n",
    "    count_idf = 0\n",
    "    tf = 0\n",
    "    idf = 0\n",
    "    tfidf = 0\n",
    "    for document in corpus:\n",
    "        count_tf += document.count(target_word)\n",
    "        if target_word in document:\n",
    "            count_idf += 1\n",
    "        else:\n",
    "            pass\n",
    "    tf = count_tf/50386 # 50386 means total number of words in text_filtered\n",
    "    idf = math.log((len(corpus)+1)/count_idf+1)\n",
    "    tfidf = idf * tf\n",
    "    return tfidf\n",
    "\n",
    "dict_with_tfidf = {}\n",
    "\n",
    "for document in text_filtered:\n",
    "    for word in document:\n",
    "        dict_with_tfidf[word] = tfidf_value(text_filtered, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these tfidf value, we could get the representation of the courses using the following codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def course_repre1(course):\n",
    "    new_word_vec = {}\n",
    "    for word in course:\n",
    "        new_word_vec[word] = dict_with_tfidf[word]*wv_model.wv[word]\n",
    "    return new_word_vec\n",
    "\n",
    "courses_repre1 = {}\n",
    "for i in range(len(text_filtered)):\n",
    "    courses_repre[i] = np.sum(list(course_repre1(text_filtered[i]).values()), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.50578339e-02,  -1.43022658e-02,   2.85251737e-02,\n",
       "        -6.54643634e-03,   2.57800967e-02,  -7.11311644e-04,\n",
       "        -1.33334622e-02,   2.54102964e-02,   1.39591079e-02,\n",
       "        -1.53986120e-03,   3.98305207e-02,   3.69832963e-02,\n",
       "        -7.97165267e-04,   2.14678664e-02,  -2.83200126e-02,\n",
       "        -8.42952076e-03,  -4.85254675e-02,  -4.62653190e-02,\n",
       "        -2.06604041e-02,   2.77011674e-02,  -9.87535715e-03,\n",
       "        -2.86755301e-02,  -6.01390982e-03,  -1.57423485e-02,\n",
       "         1.42402900e-02,  -2.06015375e-03,  -1.98526140e-02,\n",
       "         7.22721597e-05,  -1.92574449e-02,   1.24048376e-02,\n",
       "        -1.66571513e-02,  -1.63762029e-02,  -3.72686870e-02,\n",
       "         1.38383210e-02,  -1.66696180e-02,   1.69029627e-02,\n",
       "        -2.06767023e-02,   1.03469025e-02,  -5.54148108e-03,\n",
       "         8.04628618e-03,  -1.18002025e-02,  -1.45534193e-03,\n",
       "        -3.65804769e-02,   3.51642668e-02,   2.53339242e-02,\n",
       "        -4.49205935e-02,  -1.87563449e-02,   2.32702717e-02,\n",
       "         3.49012599e-03,   4.85880859e-03,   5.96646778e-02,\n",
       "        -7.92025775e-03,   7.16611072e-02,  -2.06978107e-03,\n",
       "         3.08704115e-02,  -1.55451372e-02,   3.30173075e-02,\n",
       "        -2.49169152e-02,  -5.10270931e-02,   1.28599368e-02,\n",
       "         2.98911203e-02,   7.25502754e-03,   7.59670802e-04,\n",
       "         2.43689027e-02,  -2.28094636e-03,  -6.52613817e-03,\n",
       "         6.39998866e-03,  -2.87158098e-02,   1.00753121e-02,\n",
       "         4.04603127e-03,  -1.22056454e-02,   1.59829650e-02,\n",
       "         2.13854760e-03,  -1.72848720e-03,   2.11706031e-02,\n",
       "        -7.89888296e-03,  -3.02327778e-02,   1.52134066e-02,\n",
       "         2.62094643e-02,  -3.76407569e-03,  -9.59288795e-03,\n",
       "        -2.19487920e-02,  -7.48224370e-03,   2.21561715e-02,\n",
       "         7.89213553e-03,  -2.40420573e-03,  -2.67951451e-02,\n",
       "        -7.60746282e-03,  -1.39995408e-03,   1.61371175e-02,\n",
       "         2.92684324e-02,   1.05303237e-02,  -4.68408540e-02,\n",
       "        -3.88430953e-02,   2.22789999e-02,  -1.16230212e-02,\n",
       "         2.76234146e-04,   1.65655240e-02,   5.53623866e-03,\n",
       "        -6.61185533e-02], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_repre1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it seems that word2vec method is not appropriate for this task. If we just use cosine similarity to measure the difference of two vectors, we could get the the following similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.99999595],\n",
       "       [ 0.99999595,  1.00000012]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(courses_repre1[0])\n",
    "b = list(courses_repre1[210]) \n",
    "matrix = np.array([a,b])\n",
    "\n",
    "cosine_similarity(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Writing II: Rhetorical Composing'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_name[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first course 'Writing II: Rhetorical Composing' seems to have no relationship with machine learning but from the matrix it tells us that these two courses are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible reason for this phenomena is that the training data (this coursera corpus) is too small. Hence, it would be better if we could use the word vectors generated from large corpus. So here we introduce Stanford's Glove embedding, which is developed by Stanford researchers and trained on Wikipedia data. You could download:\n",
    "\n",
    "[Glove Embedding](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "\n",
    "Here we use the 100-dimensional version of the model. We could convert the file to word2vec format as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we could load it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = models.KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.62980005e-01,   3.01409990e-01,   5.79779983e-01,\n",
       "         6.65479973e-02,   4.58350003e-01,  -1.53290004e-01,\n",
       "         4.32579994e-01,  -8.92149985e-01,   5.77470005e-01,\n",
       "         3.63750011e-01,   5.65240026e-01,  -5.62810004e-01,\n",
       "         3.56590003e-01,  -3.60960007e-01,  -9.96619985e-02,\n",
       "         5.27530015e-01,   3.88390005e-01,   9.61849988e-01,\n",
       "         1.88409999e-01,   3.07410002e-01,  -8.78419995e-01,\n",
       "        -3.24420005e-01,   1.12020004e+00,   7.51259997e-02,\n",
       "         4.26609993e-01,  -6.06509984e-01,  -1.38929993e-01,\n",
       "         4.78620008e-02,  -4.51579988e-01,   9.37229991e-02,\n",
       "         1.74630001e-01,   1.09619999e+00,  -1.00440001e+00,\n",
       "         6.38889968e-02,   3.80019993e-01,   2.11089998e-01,\n",
       "        -6.62469983e-01,  -4.07359987e-01,   8.94420028e-01,\n",
       "        -6.09740019e-01,  -1.85770005e-01,  -1.99129999e-01,\n",
       "        -6.92260027e-01,  -3.18060011e-01,  -7.85650015e-01,\n",
       "         2.38309994e-01,   1.29920006e-01,   8.77209976e-02,\n",
       "         4.32049990e-01,  -2.26620004e-01,   3.15490007e-01,\n",
       "        -3.17479998e-01,  -2.46319990e-03,   1.66150004e-01,\n",
       "         4.23579991e-01,  -1.80869997e+00,  -3.66990000e-01,\n",
       "         2.39490002e-01,   2.54579997e+00,   3.61110002e-01,\n",
       "         3.94859985e-02,   4.86070007e-01,  -3.69740009e-01,\n",
       "         5.72820008e-02,  -4.93169993e-01,   2.27650002e-01,\n",
       "         7.99660027e-01,   2.14279994e-01,   6.98109984e-01,\n",
       "         1.12619996e+00,  -1.35260001e-01,   7.19720006e-01,\n",
       "        -9.96049959e-04,  -2.68420011e-01,  -8.30380023e-01,\n",
       "         2.17800006e-01,   3.43549997e-01,   3.77310008e-01,\n",
       "        -4.02509987e-01,   3.31239998e-01,   1.25759995e+00,\n",
       "        -2.71959990e-01,  -8.60930026e-01,   9.00529996e-02,\n",
       "        -2.48760009e+00,   4.51999992e-01,   6.69449985e-01,\n",
       "        -5.46480000e-01,  -1.03239998e-01,  -1.69790000e-01,\n",
       "         5.94370008e-01,   1.12800002e+00,   7.57550001e-01,\n",
       "        -5.91600016e-02,   1.51519999e-01,  -2.83879995e-01,\n",
       "         4.94520009e-01,  -9.17029977e-01,   9.12890017e-01,\n",
       "        -3.09269994e-01], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use the pre-trained embedding from other corpus, one common issus is the **out of vocabulary(OOV)** issue, which means that the wikipedia text data set don't contain all the words in this coursera corpora.\n",
    "\n",
    "One solution to this problem is that we could use word embedding generated directly from this Coursera corpora instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def course_repre2(course):\n",
    "    new_word_vec = {}\n",
    "    for word in course:\n",
    "        if word in glove_input_file:\n",
    "            new_word_vec[word] = dict_with_tfidf[word]*model.wv[word]\n",
    "        else:\n",
    "            new_word_vec[word] = dict_with_tfidf[word]*wv_model.wv[word]\n",
    "    return new_word_vec\n",
    "\n",
    "courses_repre2 = {}\n",
    "for i in range(len(text_filtered)):\n",
    "    courses_repre2[i] = np.sum(list(course_repre2(text_filtered[i]).values()), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.99999595],\n",
       "       [ 0.99999595,  1.00000012]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(courses_repre2[0])\n",
    "b = list(courses_repre2[210]) \n",
    "matrix = np.array([a,b])\n",
    "\n",
    "cosine_similarity(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3.4 Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
