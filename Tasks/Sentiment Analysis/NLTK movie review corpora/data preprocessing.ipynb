{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Text data preparation is very important in this sentiment analysis project. In this section, firstly, we are going to load all the modules we need in this analysis and introduce the NLTK movie reviews corpora. Secondly, we store all the data in the python list. Thirdly, we briefly talk about how to erase the punctuation, contraction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modules Preparation & Movie Reviews Corpora\n",
    "\n",
    "The Python modules we are going to use in this sentiment analysis task are listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "f:\\python\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "f:\\python\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn import grid_search\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report,confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from statistics import mode\n",
    "\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from scipy.stats import expon as sp_expon\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie reviews corpora in NLTK contains 2000 movie reviews and each movie review is stored in a text file. If you want to see the raw data directly in your PC, just type **appdata** in the path and go to the file **nltk_data**. Then choose the corpora and after opening the movie_reviews file, you can see the raw text data. \n",
    "\n",
    "In this corpora, you could see half of the reviews are positive and the second half are negative. You can also get the details of this corpora just by running the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the text file names by using the fileids method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos/cv000_29590.txt', 'pos/cv001_18431.txt', 'pos/cv002_15918.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids('pos')[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for instance, if we want to get access to all the words in a text file by a file name, use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words(movie_reviews.fileids('pos')[movie_reviews.fileids('pos').index('pos/cv000_29590.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Input the Data to Python\n",
    "\n",
    "After knowing these above methods, we can put these files in a document. One thing to remember is that we should random shuffle the documents to erase the bias in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((movie_reviews.words(fileid), category))\n",
    "        \n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since we have got all the words, we need to create the features for the following analysis. The features we are going to use here are the most frequent words used in the movie reviews (We would use new features later to see if there is change in the classification accuracy). Here, the method **FreqDist** is used to list the words with their frequencies so that we can pick, for instance, first 5000 of them and use these words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())  # convert all the words to lowercase\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# Use the top 5000 words as the keys\n",
    "word_features1 = list(all_words.keys())[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have created the documents consist of all the movie review files and the word features. So the next thing is to judge whether a specific file contains featured words. An option is to create a function and decide if a word in word feature list also exists in a movie review text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features(text, word_features):\n",
    "    words = set(text)\n",
    "    featuresets = {}\n",
    "    for w in word_features:\n",
    "        featuresets[w] = (w in words)\n",
    "        \n",
    "    return featuresets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could use this function to process each text file in the documents and at last create the trainning set, the validation set(a data set which is always being used to compare the performance of different models) and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets1 = [(find_features(text = rev, word_features = word_features1),category) for (rev, category) in documents]\n",
    "\n",
    "training_set1 = featuresets1[:1800]\n",
    "validation_set1 = featuresets1[1800:1900]\n",
    "testing_set1 = featuresets1[1900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we use the first 5000 most frequent words as word features. However, this may lead to incorrect results because many nouns such as **movie** does not imply a viewer's attitude towards a movie. A more reliable approach is to only consider the adjectives and adverbs as features because these words are more close to a viewer's opinion. To reach this goal, we need to use the **pos_tag** function to tag a word's part of speech and select the adjectives and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())  # convert all the words to lowercase\n",
    "\n",
    "pos = pos_tag(all_words)\n",
    "\n",
    "adj_adv = []\n",
    "\n",
    "for w in pos:\n",
    "    if w[1][0] == 'J' or w[1][0] == 'R': # The tags of adjectives and adverbs begin with 'J' and 'R'\n",
    "        adj_adv.append(w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we are about to choose the most frequent 5000 adjectives and adverbs as word features to continue the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj_adv = nltk.FreqDist(adj_adv)\n",
    "\n",
    "# Use the top 5000 words as the keys\n",
    "word_features2 = list(adj_adv.keys())[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gaining the word_features, we construct new training set and testing set again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets2 = [(find_features(text = rev, word_features = word_features2),category) for (rev, category) in documents]\n",
    "\n",
    "training_set2 = featuresets2[:1800]\n",
    "validation_set2 = featuresets2[1800:1900]\n",
    "testing_set2 = featuresets2[1900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare the results with different features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the classification performance, we use the same classifier: logistics regression to finish this classification task. First, let's start with the most frequent 5000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_classifier accuracy rate with top 5000 most frequent words:  72.0\n"
     ]
    }
   ],
   "source": [
    "logistic_classifier = SklearnClassifier(LogisticRegression())\n",
    "logistic_classifier.train(training_set1)                           \n",
    "print('logistic_classifier accuracy rate with top 5000 most frequent words: ', \n",
    "      (nltk.classify.accuracy(logistic_classifier, validation_set1))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we use the featureset with top 5000 most frequent adj&adv, the accuracy rate is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_classifier accuracy rate with top 5000 most frequent adj&adv words:  82.0\n"
     ]
    }
   ],
   "source": [
    "logistic_classifier = SklearnClassifier(LogisticRegression())\n",
    "logistic_classifier.train(training_set2)                           \n",
    "print('logistic_classifier accuracy rate with top 5000 most frequent adj&adv words: ', \n",
    "      (nltk.classify.accuracy(logistic_classifier, validation_set2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a dramatic improve in the classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Appendix: Clean the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want our model to achieve better results in doing classification, one basic approach is to improve the quality of our raw text data. When we get a raw text data, the first thing we always should do is to clean it. In general, we could improve the quality of our text dataset by doing the following things:\n",
    "\n",
    "1. eliminate stopwords\n",
    "2. eliminate punctuation\n",
    "3. contraction (He's -> He is)\n",
    "4. conversion into lowercase\n",
    "5. stemming\n",
    "6. delete the HTML tags\n",
    "\n",
    "**Note**: We don't need to do these cleaning every time. Use some of them based on the real situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Eliminate the stopwords\n",
    "\n",
    "Stopwords like 'to' are meaningless in natural language processing. Hence, we can delete them first to get a much cleaner dataset.\n",
    "\n",
    "NLTK actually provides a great resource of stopwords in many languages. Here we use the English stopwords and eliminate the unnecessary words in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesitate', 'ask', 'questions']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "words=[\"Don't\", 'hesitate','to','ask','questions']\n",
    "[word for word in words if word not in stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK is marvelous because it contains many languages' stopwords. They can be directly used to help with our following analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'kazakh', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish'] "
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids(), end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the proportion of stopwords in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of stopwords in this text is about : 46.0 %\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\",\"I am very happy\"]\n",
    "words = []\n",
    "for sentence in text:\n",
    "    for word in word_tokenize(sentence):\n",
    "        words.append(word)\n",
    "\n",
    "lower_words = [word.lower() for word in words]\n",
    "        \n",
    "text_f = [word for word in lower_words if word in stops]\n",
    "\n",
    "proportion = len(text_f)/len(lower_words)\n",
    "print('The proportion of stopwords in this text is about :', np.ceil(proportion*100), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Eliminate punctuations\n",
    "\n",
    "Punctuations are also useless in our analysis. To delete all the punctuations, we need to use the **re** module and regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "tokenized_docs = [word_tokenize(doc) for doc in text]\n",
    "# [] means character classes. Character classes provide a way to match only one of a specific set of characters\n",
    "x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = x.sub(u'',token) # In python, r\"XXX\" means normal string. u\"XXX\" means unicode\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Contraction\n",
    "\n",
    "In English, many contractions appear like she's, we're. We could also use regular expressions to convert them into their original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Jack. He is Jim. He will participate in our team! He have played football for ten years~'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r\"won\\'t\", \"will not\"),\n",
    "    (r\"can\\'t\", \"cannot\"),\n",
    "    (r\"I\\'m\",\"I am\"),\n",
    "    (r\"ain\\'t\", 'is not'),\n",
    "    # \\g<1> are using back-references to capture part of the matched pattern\n",
    "    # \\g means referencing group content in the previous pattern. <1> means the first group. In the following case, the first group is w+\n",
    "    (r\"(\\w+)\\'ll\",\"\\g<1> will\"),\n",
    "    (r\"(\\w+)n\\'t\", \"\\g<1> not\"),\n",
    "    (r\"(\\w+)\\'ve\", \"\\g<1> have\"),\n",
    "    (r\"(\\w+)\\'s\", \"\\g<1> is\"),\n",
    "    (r\"(\\w+)\\'re\", \"\\g<1> are\"),\n",
    "    (r\"(\\w+)\\'d\", \"\\g<1> would\")\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self,patterns = replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex),repl) for (regex, repl) in replacement_patterns]\n",
    "        \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s, count) = re.subn(pattern, repl,s) # subn returns the times of replacement\n",
    "        return s\n",
    "    \n",
    "Object = RegexpReplacer()\n",
    "Object.replace(\"I'm Jack. He's Jim. He'll participate in our team! He've played football for ten years~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Conversion to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'a', 'pleasant', 'evening', '.', 'guests', ',', 'who', 'came', 'from', 'us', 'arrived', 'at', 'the', 'venue', 'food', 'was', 'tasty', '.', 'i', 'am', 'very', 'happy']\n"
     ]
    }
   ],
   "source": [
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\",\"I am very happy\"]\n",
    "words = []\n",
    "for sentence in text:\n",
    "    for word in word_tokenize(sentence):\n",
    "        words.append(word)\n",
    "\n",
    "lower_words = [word.lower() for word in words]\n",
    "print(lower_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.5 Stemming\n",
    "\n",
    "Stemming in NLP means that we treat each word's different variants as the same word. For instance for playing, played, play, we see them as a same word:play. nltk has many useful stemmers. The most well-know ones are Lancaster Stemmer and Porter Stemmer. Here we use Lancaster Stemmer to cope with this Coursera corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' it is a pleasant evening.', 'guests, who came from us arrived at the venue', 'food was tasty.', 'i am very happy']\t"
     ]
    }
   ],
   "source": [
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\",\"I am very happy\"]\n",
    "\n",
    "st = LancasterStemmer()\n",
    "text_stemmed = [st.stem(word) for word in text]\n",
    "print(text_stemmed, end = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.6 Remove the HTML Markup\n",
    "\n",
    "Some type of text data is tsv(tab delimited file), which always includes the HTML markup. We also need to erase them because these signs are useless. To accomplish this task, we need to use the BeautifulSoup module.\n",
    "\n",
    "Here we are going to use the labeled datasets in [Kaggle Movie Review Analysis](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file f:\\python\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "example1 = BeautifulSoup(labeled_train[\"review\"][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "print(labeled_train['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results shown above, we could see that the HTML markers have been delimited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the end, all the preprocessing steps could be written in the following one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Stemming\n",
    "    st = LancasterStemmer()\n",
    "    text_stemmed = [st.stem(word) for word in meaningful_words]\n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( text_stemmed ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
