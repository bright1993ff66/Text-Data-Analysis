{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the dataset. We use pandas to read the tab delimited file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "header=0: indicates that the first line of the file contains column names\n",
    "\"delimiter=\\t\": indicates that the fields are separated by tabs\n",
    "quoting=3: tells Python to ignore doubled quotes\n",
    "\"\"\"\n",
    "\n",
    "labeled_train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test_data = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sentiment', 'review'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a remainder, the following code shows the first review given by a user. From the review, we could see HTML tags, abbreviations, punctuations. We need to clean the data before putting it to a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_train['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning\n",
    "\n",
    "In this part, we are going to do the text data preprocessing, which mainly includes deleting the punctuations, removing HTML tags, removing non-letters, etc. The following function shows how to finish these data cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review, remove_stopwords = False ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    try:\n",
    "        review_text = BeautifulSoup(raw_review).get_text() \n",
    "    except TypeError:\n",
    "        review_text = \"None\"\n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))                  \n",
    "        # \n",
    "        # 5. Remove stop words\n",
    "        words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', 'moonwalker', 'is', 'part', 'biography', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'mj', 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', 'kay', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'mj', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', 'some', 'may', 'call', 'mj', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'mj', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord', 'why', 'he', 'wants', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me', 'because', 'mj', 'overheard', 'his', 'plans', 'nah', 'joe', 'pesci', 's', 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno', 'maybe', 'he', 'just', 'hates', 'mj', 's', 'music', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'mj', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', 'also', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', 'bottom', 'line', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'mj', 'on', 'one', 'level', 'or', 'another', 'which', 'i', 'think', 'is', 'most', 'people', 'if', 'not', 'then', 'stay', 'away', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', 'mj', 's', 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty', 'well', 'with', 'all', 'the', 'attention', 'i', 've', 'gave', 'this', 'subject', 'hmmm', 'well', 'i', 'don', 't', 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors', 'i', 'know', 'this', 'for', 'a', 'fact', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sickest', 'liars', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file f:\\python\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "clean_review = review_to_words( labeled_train[\"review\"][0] )\n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could run through all the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file f:\\python\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the labeled training set movie reviews...\n",
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = labeled_train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_labeled_train_reviews = []\n",
    "\n",
    "print('Cleaning the labeled training set movie reviews...')\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range(num_reviews):\n",
    "    if ((i+1) % 1000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1, num_reviews ))\n",
    "    clean_labeled_train_reviews.append( review_to_words( labeled_train[\"review\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the unlabeled training set movie reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file f:\\python\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 115764\n",
      "\n",
      "Review 2000 of 115764\n",
      "\n",
      "Review 3000 of 115764\n",
      "\n",
      "Review 4000 of 115764\n",
      "\n",
      "Review 5000 of 115764\n",
      "\n",
      "Review 6000 of 115764\n",
      "\n",
      "Review 7000 of 115764\n",
      "\n",
      "Review 8000 of 115764\n",
      "\n",
      "Review 9000 of 115764\n",
      "\n",
      "Review 10000 of 115764\n",
      "\n",
      "Review 11000 of 115764\n",
      "\n",
      "Review 12000 of 115764\n",
      "\n",
      "Review 13000 of 115764\n",
      "\n",
      "Review 14000 of 115764\n",
      "\n",
      "Review 15000 of 115764\n",
      "\n",
      "Review 16000 of 115764\n",
      "\n",
      "Review 17000 of 115764\n",
      "\n",
      "Review 18000 of 115764\n",
      "\n",
      "Review 19000 of 115764\n",
      "\n",
      "Review 20000 of 115764\n",
      "\n",
      "Review 21000 of 115764\n",
      "\n",
      "Review 22000 of 115764\n",
      "\n",
      "Review 23000 of 115764\n",
      "\n",
      "Review 24000 of 115764\n",
      "\n",
      "Review 25000 of 115764\n",
      "\n",
      "Review 26000 of 115764\n",
      "\n",
      "Review 27000 of 115764\n",
      "\n",
      "Review 28000 of 115764\n",
      "\n",
      "Review 29000 of 115764\n",
      "\n",
      "Review 30000 of 115764\n",
      "\n",
      "Review 31000 of 115764\n",
      "\n",
      "Review 32000 of 115764\n",
      "\n",
      "Review 33000 of 115764\n",
      "\n",
      "Review 34000 of 115764\n",
      "\n",
      "Review 35000 of 115764\n",
      "\n",
      "Review 36000 of 115764\n",
      "\n",
      "Review 37000 of 115764\n",
      "\n",
      "Review 38000 of 115764\n",
      "\n",
      "Review 39000 of 115764\n",
      "\n",
      "Review 40000 of 115764\n",
      "\n",
      "Review 41000 of 115764\n",
      "\n",
      "Review 42000 of 115764\n",
      "\n",
      "Review 43000 of 115764\n",
      "\n",
      "Review 44000 of 115764\n",
      "\n",
      "Review 45000 of 115764\n",
      "\n",
      "Review 46000 of 115764\n",
      "\n",
      "Review 47000 of 115764\n",
      "\n",
      "Review 48000 of 115764\n",
      "\n",
      "Review 49000 of 115764\n",
      "\n",
      "Review 50000 of 115764\n",
      "\n",
      "Review 51000 of 115764\n",
      "\n",
      "Review 52000 of 115764\n",
      "\n",
      "Review 53000 of 115764\n",
      "\n",
      "Review 54000 of 115764\n",
      "\n",
      "Review 55000 of 115764\n",
      "\n",
      "Review 56000 of 115764\n",
      "\n",
      "Review 57000 of 115764\n",
      "\n",
      "Review 58000 of 115764\n",
      "\n",
      "Review 59000 of 115764\n",
      "\n",
      "Review 60000 of 115764\n",
      "\n",
      "Review 61000 of 115764\n",
      "\n",
      "Review 62000 of 115764\n",
      "\n",
      "Review 63000 of 115764\n",
      "\n",
      "Review 64000 of 115764\n",
      "\n",
      "Review 65000 of 115764\n",
      "\n",
      "Review 66000 of 115764\n",
      "\n",
      "Review 67000 of 115764\n",
      "\n",
      "Review 68000 of 115764\n",
      "\n",
      "Review 69000 of 115764\n",
      "\n",
      "Review 70000 of 115764\n",
      "\n",
      "Review 71000 of 115764\n",
      "\n",
      "Review 72000 of 115764\n",
      "\n",
      "Review 73000 of 115764\n",
      "\n",
      "Review 74000 of 115764\n",
      "\n",
      "Review 75000 of 115764\n",
      "\n",
      "Review 76000 of 115764\n",
      "\n",
      "Review 77000 of 115764\n",
      "\n",
      "Review 78000 of 115764\n",
      "\n",
      "Review 79000 of 115764\n",
      "\n",
      "Review 80000 of 115764\n",
      "\n",
      "Review 81000 of 115764\n",
      "\n",
      "Review 82000 of 115764\n",
      "\n",
      "Review 83000 of 115764\n",
      "\n",
      "Review 84000 of 115764\n",
      "\n",
      "Review 85000 of 115764\n",
      "\n",
      "Review 86000 of 115764\n",
      "\n",
      "Review 87000 of 115764\n",
      "\n",
      "Review 88000 of 115764\n",
      "\n",
      "Review 89000 of 115764\n",
      "\n",
      "Review 90000 of 115764\n",
      "\n",
      "Review 91000 of 115764\n",
      "\n",
      "Review 92000 of 115764\n",
      "\n",
      "Review 93000 of 115764\n",
      "\n",
      "Review 94000 of 115764\n",
      "\n",
      "Review 95000 of 115764\n",
      "\n",
      "Review 96000 of 115764\n",
      "\n",
      "Review 97000 of 115764\n",
      "\n",
      "Review 98000 of 115764\n",
      "\n",
      "Review 99000 of 115764\n",
      "\n",
      "Review 100000 of 115764\n",
      "\n",
      "Review 101000 of 115764\n",
      "\n",
      "Review 102000 of 115764\n",
      "\n",
      "Review 103000 of 115764\n",
      "\n",
      "Review 104000 of 115764\n",
      "\n",
      "Review 105000 of 115764\n",
      "\n",
      "Review 106000 of 115764\n",
      "\n",
      "Review 107000 of 115764\n",
      "\n",
      "Review 108000 of 115764\n",
      "\n",
      "Review 109000 of 115764\n",
      "\n",
      "Review 110000 of 115764\n",
      "\n",
      "Review 111000 of 115764\n",
      "\n",
      "Review 112000 of 115764\n",
      "\n",
      "Review 113000 of 115764\n",
      "\n",
      "Review 114000 of 115764\n",
      "\n",
      "Review 115000 of 115764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = unlabeled_train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_unlabeled_train_reviews = []\n",
    "\n",
    "print('Cleaning the unlabeled training set movie reviews...')\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "\n",
    "\n",
    "for i in range(num_reviews):\n",
    "    if ((i+1) % 1000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1, num_reviews ))\n",
    "    clean_unlabeled_train_reviews.append( review_to_words( unlabeled_train[\"review\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the test dataset movie reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file f:\\python\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = test_data[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_test_data_reviews = []\n",
    "\n",
    "print('Cleaning the test dataset movie reviews...')\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "\n",
    "for i in range(num_reviews):\n",
    "    if ((i+1) % 1000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1, num_reviews ))\n",
    "    clean_test_data_reviews.append( review_to_words( test_data[\"review\"][i] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Construct the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, to do the sentiment classification task, we want to first use word embedding to generate the representation of each word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing the sentences from the labeled training set....\n",
      "Parsing the sentences from the unlabeled training set....\n",
      "Time taken for getting all the text:  0.03859972953796387  seconds\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print('Parsing the sentences from the labeled training set....')\n",
    "start = time.time()\n",
    "\n",
    "for review in clean_labeled_train_reviews:\n",
    "    sentences.append(review)\n",
    "    \n",
    "print('Parsing the sentences from the unlabeled training set....')\n",
    "\n",
    "for review in clean_unlabeled_train_reviews:\n",
    "    sentences.append(review)\n",
    "    \n",
    "end = time.time()\n",
    "duration = end - start\n",
    "\n",
    "print(\"Time taken for getting all the text: \", duration, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have got all the words in the labeled and unlabeled dataset, we could use the word2vec model in [gensim](https://radimrehurek.com/gensim/models/word2vec.html) to generate the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-11 09:15:33,933 : INFO : collecting all words and their counts\n",
      "2018-03-11 09:15:33,935 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-11 09:15:34,486 : INFO : PROGRESS: at sentence #10000, processed 2385575 words, keeping 51527 word types\n",
      "2018-03-11 09:15:35,011 : INFO : PROGRESS: at sentence #20000, processed 4747504 words, keeping 67813 word types\n",
      "2018-03-11 09:15:35,517 : INFO : PROGRESS: at sentence #30000, processed 7109778 words, keeping 81728 word types\n",
      "2018-03-11 09:15:36,038 : INFO : PROGRESS: at sentence #40000, processed 9467206 words, keeping 92655 word types\n",
      "2018-03-11 09:15:36,562 : INFO : PROGRESS: at sentence #50000, processed 11825236 words, keeping 99583 word types\n",
      "2018-03-11 09:15:37,077 : INFO : PROGRESS: at sentence #60000, processed 14190901 words, keeping 105913 word types\n",
      "2018-03-11 09:15:37,597 : INFO : PROGRESS: at sentence #70000, processed 16573274 words, keeping 111433 word types\n",
      "2018-03-11 09:15:38,125 : INFO : PROGRESS: at sentence #80000, processed 18978383 words, keeping 116764 word types\n",
      "2018-03-11 09:15:38,654 : INFO : PROGRESS: at sentence #90000, processed 21360809 words, keeping 121461 word types\n",
      "2018-03-11 09:15:39,184 : INFO : PROGRESS: at sentence #100000, processed 23756442 words, keeping 123558 word types\n",
      "2018-03-11 09:15:39,763 : INFO : PROGRESS: at sentence #110000, processed 26138772 words, keeping 123567 word types\n",
      "2018-03-11 09:15:40,388 : INFO : PROGRESS: at sentence #120000, processed 28498204 words, keeping 123577 word types\n",
      "2018-03-11 09:15:40,943 : INFO : PROGRESS: at sentence #130000, processed 30857061 words, keeping 123587 word types\n",
      "2018-03-11 09:15:41,480 : INFO : PROGRESS: at sentence #140000, processed 33230318 words, keeping 123590 word types\n",
      "2018-03-11 09:15:41,532 : INFO : collected 123590 word types from a corpus of 33409920 raw words and 140764 sentences\n",
      "2018-03-11 09:15:41,533 : INFO : Loading a fresh vocabulary\n",
      "2018-03-11 09:15:41,681 : INFO : min_count=40 retains 23261 unique words (18% of original 123590, drops 100329)\n",
      "2018-03-11 09:15:41,682 : INFO : min_count=40 leaves 32733641 word corpus (97% of original 33409920, drops 676279)\n",
      "2018-03-11 09:15:41,801 : INFO : deleting the raw counts dictionary of 123590 items\n",
      "2018-03-11 09:15:41,809 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-03-11 09:15:41,810 : INFO : downsampling leaves estimated 24350254 word corpus (74.4% of prior 32733641)\n",
      "2018-03-11 09:15:41,812 : INFO : estimated required memory for 23261 words and 300 dimensions: 67456900 bytes\n",
      "2018-03-11 09:15:41,961 : INFO : resetting layer weights\n",
      "2018-03-11 09:15:42,400 : INFO : training model with 4 workers on 23261 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-03-11 09:15:43,447 : INFO : PROGRESS: at 0.39% examples, 472149 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:15:44,448 : INFO : PROGRESS: at 0.80% examples, 488672 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:15:45,452 : INFO : PROGRESS: at 1.23% examples, 499018 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-11 09:15:46,458 : INFO : PROGRESS: at 1.70% examples, 516520 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:15:47,467 : INFO : PROGRESS: at 2.18% examples, 527712 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:15:48,476 : INFO : PROGRESS: at 2.61% examples, 525896 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:15:49,485 : INFO : PROGRESS: at 3.02% examples, 521633 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:15:50,497 : INFO : PROGRESS: at 3.39% examples, 510383 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:15:51,507 : INFO : PROGRESS: at 3.84% examples, 513464 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:15:52,519 : INFO : PROGRESS: at 4.30% examples, 517363 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:15:53,549 : INFO : PROGRESS: at 4.71% examples, 514639 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:15:54,556 : INFO : PROGRESS: at 5.11% examples, 511377 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:15:55,564 : INFO : PROGRESS: at 5.48% examples, 505447 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:15:56,585 : INFO : PROGRESS: at 5.81% examples, 497848 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:15:57,586 : INFO : PROGRESS: at 6.24% examples, 498534 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:15:58,596 : INFO : PROGRESS: at 6.62% examples, 496268 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:15:59,601 : INFO : PROGRESS: at 7.01% examples, 495222 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:00,609 : INFO : PROGRESS: at 7.40% examples, 493449 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:01,614 : INFO : PROGRESS: at 7.77% examples, 491065 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:02,650 : INFO : PROGRESS: at 8.17% examples, 489602 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:03,663 : INFO : PROGRESS: at 8.56% examples, 488749 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:04,682 : INFO : PROGRESS: at 8.98% examples, 489536 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:05,705 : INFO : PROGRESS: at 9.34% examples, 486524 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:06,708 : INFO : PROGRESS: at 9.70% examples, 484955 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:07,709 : INFO : PROGRESS: at 10.07% examples, 483837 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:08,712 : INFO : PROGRESS: at 10.45% examples, 483306 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:09,713 : INFO : PROGRESS: at 10.84% examples, 482679 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:10,722 : INFO : PROGRESS: at 11.24% examples, 482894 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:11,729 : INFO : PROGRESS: at 11.57% examples, 480647 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:12,738 : INFO : PROGRESS: at 11.97% examples, 480755 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:13,761 : INFO : PROGRESS: at 12.36% examples, 479884 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:14,773 : INFO : PROGRESS: at 12.77% examples, 480601 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:15,785 : INFO : PROGRESS: at 13.18% examples, 481241 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:16,788 : INFO : PROGRESS: at 13.60% examples, 482047 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:17,794 : INFO : PROGRESS: at 14.01% examples, 482664 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:18,819 : INFO : PROGRESS: at 14.43% examples, 483206 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:19,845 : INFO : PROGRESS: at 14.85% examples, 483776 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:20,856 : INFO : PROGRESS: at 15.28% examples, 484463 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:21,862 : INFO : PROGRESS: at 15.67% examples, 484292 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:22,888 : INFO : PROGRESS: at 16.03% examples, 482665 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:23,893 : INFO : PROGRESS: at 16.42% examples, 482177 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:16:24,909 : INFO : PROGRESS: at 16.84% examples, 482748 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:25,931 : INFO : PROGRESS: at 17.21% examples, 481981 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:26,931 : INFO : PROGRESS: at 17.58% examples, 480854 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:27,947 : INFO : PROGRESS: at 17.91% examples, 479074 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:28,949 : INFO : PROGRESS: at 18.23% examples, 477218 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:29,964 : INFO : PROGRESS: at 18.59% examples, 476236 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:30,968 : INFO : PROGRESS: at 18.98% examples, 475991 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:31,975 : INFO : PROGRESS: at 19.36% examples, 475709 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:32,989 : INFO : PROGRESS: at 19.72% examples, 474923 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:33,990 : INFO : PROGRESS: at 20.12% examples, 475000 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:34,994 : INFO : PROGRESS: at 20.49% examples, 474800 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:36,000 : INFO : PROGRESS: at 20.88% examples, 474700 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:37,001 : INFO : PROGRESS: at 21.25% examples, 474233 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:38,013 : INFO : PROGRESS: at 21.67% examples, 474857 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-11 09:16:39,018 : INFO : PROGRESS: at 22.03% examples, 474240 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:40,022 : INFO : PROGRESS: at 22.46% examples, 474784 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:41,026 : INFO : PROGRESS: at 22.88% examples, 475309 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:42,054 : INFO : PROGRESS: at 23.32% examples, 475883 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:43,061 : INFO : PROGRESS: at 23.75% examples, 476595 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:44,070 : INFO : PROGRESS: at 24.16% examples, 476905 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:45,074 : INFO : PROGRESS: at 24.54% examples, 476710 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:46,088 : INFO : PROGRESS: at 24.91% examples, 476083 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-11 09:16:47,103 : INFO : PROGRESS: at 25.30% examples, 475807 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-11 09:16:48,120 : INFO : PROGRESS: at 25.70% examples, 475887 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:49,136 : INFO : PROGRESS: at 26.09% examples, 475717 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:50,142 : INFO : PROGRESS: at 26.51% examples, 475956 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:51,148 : INFO : PROGRESS: at 26.91% examples, 476194 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-11 09:16:52,151 : INFO : PROGRESS: at 27.31% examples, 476323 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:53,161 : INFO : PROGRESS: at 27.64% examples, 475190 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:54,169 : INFO : PROGRESS: at 27.98% examples, 474303 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:55,186 : INFO : PROGRESS: at 28.36% examples, 473964 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:56,188 : INFO : PROGRESS: at 28.75% examples, 474098 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:57,199 : INFO : PROGRESS: at 29.09% examples, 473157 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:16:58,206 : INFO : PROGRESS: at 29.48% examples, 472927 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:16:59,208 : INFO : PROGRESS: at 29.78% examples, 471687 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:00,242 : INFO : PROGRESS: at 30.13% examples, 471094 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:01,247 : INFO : PROGRESS: at 30.49% examples, 470711 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:02,248 : INFO : PROGRESS: at 30.89% examples, 470839 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:03,252 : INFO : PROGRESS: at 31.28% examples, 470904 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:04,262 : INFO : PROGRESS: at 31.68% examples, 471203 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:05,276 : INFO : PROGRESS: at 32.05% examples, 470898 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:06,287 : INFO : PROGRESS: at 32.34% examples, 469387 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:07,297 : INFO : PROGRESS: at 32.70% examples, 469039 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:08,318 : INFO : PROGRESS: at 33.07% examples, 468800 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:09,318 : INFO : PROGRESS: at 33.40% examples, 467955 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:10,325 : INFO : PROGRESS: at 33.71% examples, 466994 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:11,325 : INFO : PROGRESS: at 34.05% examples, 466474 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:12,348 : INFO : PROGRESS: at 34.34% examples, 465132 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:13,360 : INFO : PROGRESS: at 34.66% examples, 464304 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:14,386 : INFO : PROGRESS: at 35.06% examples, 464332 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:15,410 : INFO : PROGRESS: at 35.45% examples, 464393 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:16,445 : INFO : PROGRESS: at 35.82% examples, 464098 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:17,454 : INFO : PROGRESS: at 36.21% examples, 464075 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:17:18,457 : INFO : PROGRESS: at 36.60% examples, 464125 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:19,462 : INFO : PROGRESS: at 37.02% examples, 464639 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:20,466 : INFO : PROGRESS: at 37.43% examples, 464951 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:21,473 : INFO : PROGRESS: at 37.87% examples, 465432 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-11 09:17:22,474 : INFO : PROGRESS: at 38.28% examples, 465928 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:23,495 : INFO : PROGRESS: at 38.71% examples, 466325 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:24,502 : INFO : PROGRESS: at 39.14% examples, 466781 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:25,503 : INFO : PROGRESS: at 39.55% examples, 467101 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:26,504 : INFO : PROGRESS: at 39.90% examples, 466724 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:27,528 : INFO : PROGRESS: at 40.27% examples, 466551 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:28,535 : INFO : PROGRESS: at 40.67% examples, 466718 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:29,554 : INFO : PROGRESS: at 41.01% examples, 466145 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:30,557 : INFO : PROGRESS: at 41.34% examples, 465584 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:31,570 : INFO : PROGRESS: at 41.70% examples, 465197 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:32,584 : INFO : PROGRESS: at 42.06% examples, 464988 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:33,598 : INFO : PROGRESS: at 42.43% examples, 464742 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:34,605 : INFO : PROGRESS: at 42.76% examples, 464130 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:17:35,609 : INFO : PROGRESS: at 43.05% examples, 463115 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:36,650 : INFO : PROGRESS: at 43.34% examples, 461839 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:37,652 : INFO : PROGRESS: at 43.65% examples, 461048 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:38,657 : INFO : PROGRESS: at 44.00% examples, 460693 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:39,687 : INFO : PROGRESS: at 44.32% examples, 460007 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:40,736 : INFO : PROGRESS: at 44.64% examples, 459211 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:41,764 : INFO : PROGRESS: at 44.97% examples, 458608 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:42,767 : INFO : PROGRESS: at 45.34% examples, 458533 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:43,769 : INFO : PROGRESS: at 45.70% examples, 458304 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:44,772 : INFO : PROGRESS: at 46.09% examples, 458342 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:45,783 : INFO : PROGRESS: at 46.47% examples, 458305 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:46,783 : INFO : PROGRESS: at 46.83% examples, 458255 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:47,803 : INFO : PROGRESS: at 47.16% examples, 457621 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:48,817 : INFO : PROGRESS: at 47.50% examples, 457297 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:49,822 : INFO : PROGRESS: at 47.88% examples, 457220 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:50,832 : INFO : PROGRESS: at 48.23% examples, 456972 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:51,844 : INFO : PROGRESS: at 48.58% examples, 456710 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:52,853 : INFO : PROGRESS: at 48.94% examples, 456584 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:53,870 : INFO : PROGRESS: at 49.32% examples, 456487 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:54,870 : INFO : PROGRESS: at 49.65% examples, 456164 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:55,886 : INFO : PROGRESS: at 50.04% examples, 456276 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:17:56,893 : INFO : PROGRESS: at 50.41% examples, 456208 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:57,899 : INFO : PROGRESS: at 50.76% examples, 455992 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:17:58,910 : INFO : PROGRESS: at 51.12% examples, 455825 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-11 09:17:59,961 : INFO : PROGRESS: at 51.40% examples, 454926 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:00,964 : INFO : PROGRESS: at 51.75% examples, 454784 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:01,974 : INFO : PROGRESS: at 52.11% examples, 454619 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:02,994 : INFO : PROGRESS: at 52.46% examples, 454320 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-11 09:18:04,012 : INFO : PROGRESS: at 52.84% examples, 454346 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:05,032 : INFO : PROGRESS: at 53.19% examples, 454103 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:06,071 : INFO : PROGRESS: at 53.55% examples, 453868 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:07,099 : INFO : PROGRESS: at 53.89% examples, 453559 words/s, in_qsize 8, out_qsize 2\n",
      "2018-03-11 09:18:08,105 : INFO : PROGRESS: at 54.23% examples, 453270 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:09,125 : INFO : PROGRESS: at 54.57% examples, 453001 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:18:10,145 : INFO : PROGRESS: at 54.94% examples, 452889 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:11,161 : INFO : PROGRESS: at 55.25% examples, 452390 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:12,169 : INFO : PROGRESS: at 55.59% examples, 452127 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:13,175 : INFO : PROGRESS: at 55.97% examples, 452163 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:14,180 : INFO : PROGRESS: at 56.34% examples, 452089 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:18:15,180 : INFO : PROGRESS: at 56.74% examples, 452258 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:16,190 : INFO : PROGRESS: at 57.11% examples, 452223 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:17,203 : INFO : PROGRESS: at 57.41% examples, 451639 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:18,218 : INFO : PROGRESS: at 57.69% examples, 450822 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:19,263 : INFO : PROGRESS: at 57.95% examples, 449861 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:18:20,285 : INFO : PROGRESS: at 58.23% examples, 449131 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:21,291 : INFO : PROGRESS: at 58.53% examples, 448549 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:22,324 : INFO : PROGRESS: at 58.82% examples, 447846 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:23,356 : INFO : PROGRESS: at 59.14% examples, 447426 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:24,391 : INFO : PROGRESS: at 59.47% examples, 447080 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:25,398 : INFO : PROGRESS: at 59.77% examples, 446593 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:26,398 : INFO : PROGRESS: at 60.09% examples, 446185 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:27,401 : INFO : PROGRESS: at 60.41% examples, 445826 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:28,437 : INFO : PROGRESS: at 60.72% examples, 445366 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:29,468 : INFO : PROGRESS: at 61.09% examples, 445310 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:30,477 : INFO : PROGRESS: at 61.45% examples, 445195 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:31,484 : INFO : PROGRESS: at 61.80% examples, 445117 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:32,496 : INFO : PROGRESS: at 62.11% examples, 444690 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:18:33,511 : INFO : PROGRESS: at 62.41% examples, 444147 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:34,517 : INFO : PROGRESS: at 62.78% examples, 444119 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:35,548 : INFO : PROGRESS: at 63.11% examples, 443831 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:36,566 : INFO : PROGRESS: at 63.50% examples, 443899 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:37,573 : INFO : PROGRESS: at 63.83% examples, 443598 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:38,576 : INFO : PROGRESS: at 64.17% examples, 443428 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:39,600 : INFO : PROGRESS: at 64.51% examples, 443211 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:40,617 : INFO : PROGRESS: at 64.88% examples, 443216 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:18:41,646 : INFO : PROGRESS: at 65.24% examples, 443075 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:42,649 : INFO : PROGRESS: at 65.62% examples, 443113 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:43,674 : INFO : PROGRESS: at 65.97% examples, 442939 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:18:44,700 : INFO : PROGRESS: at 66.29% examples, 442568 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:45,714 : INFO : PROGRESS: at 66.59% examples, 442076 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:46,735 : INFO : PROGRESS: at 66.86% examples, 441457 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-11 09:18:47,754 : INFO : PROGRESS: at 67.20% examples, 441275 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:48,761 : INFO : PROGRESS: at 67.60% examples, 441463 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:49,770 : INFO : PROGRESS: at 67.99% examples, 441611 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:50,805 : INFO : PROGRESS: at 68.38% examples, 441692 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-11 09:18:51,814 : INFO : PROGRESS: at 68.76% examples, 441822 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:52,816 : INFO : PROGRESS: at 69.12% examples, 441761 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:53,859 : INFO : PROGRESS: at 69.48% examples, 441637 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-11 09:18:54,875 : INFO : PROGRESS: at 69.86% examples, 441794 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:55,883 : INFO : PROGRESS: at 70.18% examples, 441517 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:18:56,894 : INFO : PROGRESS: at 70.53% examples, 441427 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:57,920 : INFO : PROGRESS: at 70.89% examples, 441316 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:58,931 : INFO : PROGRESS: at 71.22% examples, 441154 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:18:59,934 : INFO : PROGRESS: at 71.59% examples, 441293 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:00,946 : INFO : PROGRESS: at 71.97% examples, 441362 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:01,951 : INFO : PROGRESS: at 72.34% examples, 441392 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:02,954 : INFO : PROGRESS: at 72.72% examples, 441468 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:03,981 : INFO : PROGRESS: at 73.02% examples, 441072 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:04,991 : INFO : PROGRESS: at 73.39% examples, 441103 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:06,005 : INFO : PROGRESS: at 73.73% examples, 440946 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:07,012 : INFO : PROGRESS: at 74.08% examples, 440903 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:08,018 : INFO : PROGRESS: at 74.43% examples, 440867 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:09,029 : INFO : PROGRESS: at 74.83% examples, 441041 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:10,045 : INFO : PROGRESS: at 75.22% examples, 441187 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:11,062 : INFO : PROGRESS: at 75.62% examples, 441344 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:12,070 : INFO : PROGRESS: at 76.00% examples, 441451 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:13,103 : INFO : PROGRESS: at 76.37% examples, 441388 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:14,131 : INFO : PROGRESS: at 76.75% examples, 441403 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:15,143 : INFO : PROGRESS: at 77.10% examples, 441328 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:16,159 : INFO : PROGRESS: at 77.42% examples, 441048 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-11 09:19:17,181 : INFO : PROGRESS: at 77.79% examples, 440988 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:18,204 : INFO : PROGRESS: at 78.15% examples, 440920 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:19,211 : INFO : PROGRESS: at 78.49% examples, 440796 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:20,217 : INFO : PROGRESS: at 78.87% examples, 440893 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-11 09:19:21,224 : INFO : PROGRESS: at 79.24% examples, 440895 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:22,225 : INFO : PROGRESS: at 79.62% examples, 441030 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:23,224 : INFO : PROGRESS: at 80.00% examples, 441138 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-11 09:19:24,234 : INFO : PROGRESS: at 80.39% examples, 441236 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:25,248 : INFO : PROGRESS: at 80.78% examples, 441440 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:26,290 : INFO : PROGRESS: at 81.18% examples, 441529 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:27,305 : INFO : PROGRESS: at 81.57% examples, 441664 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:28,320 : INFO : PROGRESS: at 81.96% examples, 441793 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:29,337 : INFO : PROGRESS: at 82.36% examples, 441894 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:30,343 : INFO : PROGRESS: at 82.74% examples, 441981 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:31,360 : INFO : PROGRESS: at 83.13% examples, 442083 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:32,374 : INFO : PROGRESS: at 83.53% examples, 442182 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:33,381 : INFO : PROGRESS: at 83.92% examples, 442304 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:34,384 : INFO : PROGRESS: at 84.30% examples, 442401 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:35,385 : INFO : PROGRESS: at 84.69% examples, 442504 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:36,404 : INFO : PROGRESS: at 85.04% examples, 442380 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:37,426 : INFO : PROGRESS: at 85.40% examples, 442316 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:38,442 : INFO : PROGRESS: at 85.73% examples, 442116 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:39,447 : INFO : PROGRESS: at 86.12% examples, 442234 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:40,473 : INFO : PROGRESS: at 86.51% examples, 442286 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:41,476 : INFO : PROGRESS: at 86.88% examples, 442320 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:42,493 : INFO : PROGRESS: at 87.26% examples, 442358 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-11 09:19:43,495 : INFO : PROGRESS: at 87.65% examples, 442510 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:44,500 : INFO : PROGRESS: at 88.03% examples, 442535 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:45,520 : INFO : PROGRESS: at 88.38% examples, 442415 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:46,528 : INFO : PROGRESS: at 88.75% examples, 442516 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:47,530 : INFO : PROGRESS: at 89.15% examples, 442640 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:48,556 : INFO : PROGRESS: at 89.48% examples, 442429 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:49,589 : INFO : PROGRESS: at 89.86% examples, 442486 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:50,608 : INFO : PROGRESS: at 90.25% examples, 442624 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:51,627 : INFO : PROGRESS: at 90.57% examples, 442397 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:19:52,640 : INFO : PROGRESS: at 90.93% examples, 442324 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:53,642 : INFO : PROGRESS: at 91.28% examples, 442323 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:54,652 : INFO : PROGRESS: at 91.61% examples, 442219 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:19:55,656 : INFO : PROGRESS: at 91.96% examples, 442083 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:56,668 : INFO : PROGRESS: at 92.32% examples, 442060 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-11 09:19:57,681 : INFO : PROGRESS: at 92.70% examples, 442105 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:19:58,698 : INFO : PROGRESS: at 93.06% examples, 442140 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:19:59,721 : INFO : PROGRESS: at 93.43% examples, 442082 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:00,728 : INFO : PROGRESS: at 93.73% examples, 441828 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:01,732 : INFO : PROGRESS: at 94.05% examples, 441632 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:02,738 : INFO : PROGRESS: at 94.43% examples, 441708 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:20:03,752 : INFO : PROGRESS: at 94.82% examples, 441810 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:04,761 : INFO : PROGRESS: at 95.22% examples, 441964 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:05,767 : INFO : PROGRESS: at 95.61% examples, 442077 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:06,800 : INFO : PROGRESS: at 96.01% examples, 442200 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:07,810 : INFO : PROGRESS: at 96.42% examples, 442374 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:08,816 : INFO : PROGRESS: at 96.82% examples, 442501 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:20:09,835 : INFO : PROGRESS: at 97.21% examples, 442640 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:10,845 : INFO : PROGRESS: at 97.62% examples, 442770 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:11,849 : INFO : PROGRESS: at 98.01% examples, 442898 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:12,857 : INFO : PROGRESS: at 98.41% examples, 443052 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:13,859 : INFO : PROGRESS: at 98.80% examples, 443157 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-11 09:20:14,884 : INFO : PROGRESS: at 99.20% examples, 443278 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:20:15,887 : INFO : PROGRESS: at 99.59% examples, 443401 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-11 09:20:16,880 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-11 09:20:16,903 : INFO : PROGRESS: at 99.99% examples, 443520 words/s, in_qsize 2, out_qsize 1\n",
      "2018-03-11 09:20:16,905 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-11 09:20:16,916 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-11 09:20:16,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-11 09:20:16,921 : INFO : training on 167049600 raw words (121747589 effective words) took 274.5s, 443543 effective words/s\n",
      "2018-03-11 09:20:16,923 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-03-11 09:20:17,314 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-03-11 09:20:17,316 : INFO : not storing attribute syn0norm\n",
      "2018-03-11 09:20:17,317 : INFO : not storing attribute cum_table\n",
      "2018-03-11 09:20:19,102 : INFO : saved 300features_40minwords_10context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for training the word vectors:  285.1712746620178  seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "print(\"Training model...\")\n",
    "start = time.time()\n",
    "model = Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "\n",
    "print(\"Total time for training the word vectors: \", duration, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got the word vector model. To load this model, you could use the following code:\n",
    "\n",
    "```Python\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-11 11:07:51,047 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2018-03-11 11:07:52,859 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2018-03-11 11:07:52,861 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-03-11 11:07:52,862 : INFO : setting ignored attribute cum_table to None\n",
      "2018-03-11 11:07:52,864 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(sentence, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((300,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    #\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    # Loop over each word in the review\n",
    "    for word in sentence:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "        if counter % 1000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[int(counter),] = makeFeatureVec(review, model, num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for training reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "print(\"Creating average feature vecs for training reviews\")\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_labeled_train_reviews, model, 300)\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_data_reviews, model, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit( trainDataVecs, labeled_train[\"sentiment\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test_data[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
