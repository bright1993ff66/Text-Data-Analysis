{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the dataset. We use pandas to read the tab delimited file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "header=0: indicates that the first line of the file contains column names\n",
    "\"delimiter=\\t\": indicates that the fields are separated by tabs\n",
    "quoting=3: tells Python to ignore doubled quotes\n",
    "\"\"\"\n",
    "\n",
    "labeled_train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test_data = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sentiment', 'review'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a remainder, the following code shows the first review given by a user. From the review, we could see HTML tags, abbreviations, punctuations. We need to clean the data before putting it to a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_train['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning\n",
    "\n",
    "In this part, we are going to do the text data preprocessing, which mainly includes deleting the punctuations, removing HTML tags, removing non-letters, etc. The following function shows how to finish these data cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review, remove_stopwords = False ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    try:\n",
    "        review_text = BeautifulSoup(raw_review).get_text() \n",
    "    except TypeError:\n",
    "        review_text = \"None\"\n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))                  \n",
    "        # \n",
    "        # 5. Remove stop words\n",
    "        words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', 'moonwalker', 'is', 'part', 'biography', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'mj', 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', 'kay', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'mj', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', 'some', 'may', 'call', 'mj', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'mj', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord', 'why', 'he', 'wants', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me', 'because', 'mj', 'overheard', 'his', 'plans', 'nah', 'joe', 'pesci', 's', 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno', 'maybe', 'he', 'just', 'hates', 'mj', 's', 'music', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'mj', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', 'also', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', 'bottom', 'line', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'mj', 'on', 'one', 'level', 'or', 'another', 'which', 'i', 'think', 'is', 'most', 'people', 'if', 'not', 'then', 'stay', 'away', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', 'mj', 's', 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty', 'well', 'with', 'all', 'the', 'attention', 'i', 've', 'gave', 'this', 'subject', 'hmmm', 'well', 'i', 'don', 't', 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors', 'i', 'know', 'this', 'for', 'a', 'fact', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sickest', 'liars', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file f:\\python\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "clean_review = review_to_words( labeled_train[\"review\"][0] )\n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could run through all the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the labeled training set movie reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file f:\\python\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = labeled_train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_labeled_train_reviews = []\n",
    "\n",
    "print('Cleaning the labeled training set movie reviews...')\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range(num_reviews):\n",
    "    if ((i+1) % 1000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1, num_reviews ))\n",
    "    clean_labeled_train_reviews.append( review_to_words( labeled_train[\"review\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the labeled training set movie reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file f:\\python\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 115764\n",
      "\n",
      "Review 2000 of 115764\n",
      "\n",
      "Review 3000 of 115764\n",
      "\n",
      "Review 4000 of 115764\n",
      "\n",
      "Review 5000 of 115764\n",
      "\n",
      "Review 6000 of 115764\n",
      "\n",
      "Review 7000 of 115764\n",
      "\n",
      "Review 8000 of 115764\n",
      "\n",
      "Review 9000 of 115764\n",
      "\n",
      "Review 10000 of 115764\n",
      "\n",
      "Review 11000 of 115764\n",
      "\n",
      "Review 12000 of 115764\n",
      "\n",
      "Review 13000 of 115764\n",
      "\n",
      "Review 14000 of 115764\n",
      "\n",
      "Review 15000 of 115764\n",
      "\n",
      "Review 16000 of 115764\n",
      "\n",
      "Review 17000 of 115764\n",
      "\n",
      "Review 18000 of 115764\n",
      "\n",
      "Review 19000 of 115764\n",
      "\n",
      "Review 20000 of 115764\n",
      "\n",
      "Review 21000 of 115764\n",
      "\n",
      "Review 22000 of 115764\n",
      "\n",
      "Review 23000 of 115764\n",
      "\n",
      "Review 24000 of 115764\n",
      "\n",
      "Review 25000 of 115764\n",
      "\n",
      "Review 26000 of 115764\n",
      "\n",
      "Review 27000 of 115764\n",
      "\n",
      "Review 28000 of 115764\n",
      "\n",
      "Review 29000 of 115764\n",
      "\n",
      "Review 30000 of 115764\n",
      "\n",
      "Review 31000 of 115764\n",
      "\n",
      "Review 32000 of 115764\n",
      "\n",
      "Review 33000 of 115764\n",
      "\n",
      "Review 34000 of 115764\n",
      "\n",
      "Review 35000 of 115764\n",
      "\n",
      "Review 36000 of 115764\n",
      "\n",
      "Review 37000 of 115764\n",
      "\n",
      "Review 38000 of 115764\n",
      "\n",
      "Review 39000 of 115764\n",
      "\n",
      "Review 40000 of 115764\n",
      "\n",
      "Review 41000 of 115764\n",
      "\n",
      "Review 42000 of 115764\n",
      "\n",
      "Review 43000 of 115764\n",
      "\n",
      "Review 44000 of 115764\n",
      "\n",
      "Review 45000 of 115764\n",
      "\n",
      "Review 46000 of 115764\n",
      "\n",
      "Review 47000 of 115764\n",
      "\n",
      "Review 48000 of 115764\n",
      "\n",
      "Review 49000 of 115764\n",
      "\n",
      "Review 50000 of 115764\n",
      "\n",
      "Review 51000 of 115764\n",
      "\n",
      "Review 52000 of 115764\n",
      "\n",
      "Review 53000 of 115764\n",
      "\n",
      "Review 54000 of 115764\n",
      "\n",
      "Review 55000 of 115764\n",
      "\n",
      "Review 56000 of 115764\n",
      "\n",
      "Review 57000 of 115764\n",
      "\n",
      "Review 58000 of 115764\n",
      "\n",
      "Review 59000 of 115764\n",
      "\n",
      "Review 60000 of 115764\n",
      "\n",
      "Review 61000 of 115764\n",
      "\n",
      "Review 62000 of 115764\n",
      "\n",
      "Review 63000 of 115764\n",
      "\n",
      "Review 64000 of 115764\n",
      "\n",
      "Review 65000 of 115764\n",
      "\n",
      "Review 66000 of 115764\n",
      "\n",
      "Review 67000 of 115764\n",
      "\n",
      "Review 68000 of 115764\n",
      "\n",
      "Review 69000 of 115764\n",
      "\n",
      "Review 70000 of 115764\n",
      "\n",
      "Review 71000 of 115764\n",
      "\n",
      "Review 72000 of 115764\n",
      "\n",
      "Review 73000 of 115764\n",
      "\n",
      "Review 74000 of 115764\n",
      "\n",
      "Review 75000 of 115764\n",
      "\n",
      "Review 76000 of 115764\n",
      "\n",
      "Review 77000 of 115764\n",
      "\n",
      "Review 78000 of 115764\n",
      "\n",
      "Review 79000 of 115764\n",
      "\n",
      "Review 80000 of 115764\n",
      "\n",
      "Review 81000 of 115764\n",
      "\n",
      "Review 82000 of 115764\n",
      "\n",
      "Review 83000 of 115764\n",
      "\n",
      "Review 84000 of 115764\n",
      "\n",
      "Review 85000 of 115764\n",
      "\n",
      "Review 86000 of 115764\n",
      "\n",
      "Review 87000 of 115764\n",
      "\n",
      "Review 88000 of 115764\n",
      "\n",
      "Review 89000 of 115764\n",
      "\n",
      "Review 90000 of 115764\n",
      "\n",
      "Review 91000 of 115764\n",
      "\n",
      "Review 92000 of 115764\n",
      "\n",
      "Review 93000 of 115764\n",
      "\n",
      "Review 94000 of 115764\n",
      "\n",
      "Review 95000 of 115764\n",
      "\n",
      "Review 96000 of 115764\n",
      "\n",
      "Review 97000 of 115764\n",
      "\n",
      "Review 98000 of 115764\n",
      "\n",
      "Review 99000 of 115764\n",
      "\n",
      "Review 100000 of 115764\n",
      "\n",
      "Review 101000 of 115764\n",
      "\n",
      "Review 102000 of 115764\n",
      "\n",
      "Review 103000 of 115764\n",
      "\n",
      "Review 104000 of 115764\n",
      "\n",
      "Review 105000 of 115764\n",
      "\n",
      "Review 106000 of 115764\n",
      "\n",
      "Review 107000 of 115764\n",
      "\n",
      "Review 108000 of 115764\n",
      "\n",
      "Review 109000 of 115764\n",
      "\n",
      "Review 110000 of 115764\n",
      "\n",
      "Review 111000 of 115764\n",
      "\n",
      "Review 112000 of 115764\n",
      "\n",
      "Review 113000 of 115764\n",
      "\n",
      "Review 114000 of 115764\n",
      "\n",
      "Review 115000 of 115764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = unlabeled_train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_unlabeled_train_reviews = []\n",
    "\n",
    "print('Cleaning the unlabeled training set movie reviews...')\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "\n",
    "\n",
    "for i in range(num_reviews):\n",
    "    if ((i+1) % 1000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1, num_reviews ))\n",
    "    clean_unlabeled_train_reviews.append( review_to_words( unlabeled_train[\"review\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the test dataset movie reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file f:\\python\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = test_data[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_test_data_reviews = []\n",
    "\n",
    "print('Cleaning the test dataset movie reviews...')\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "\n",
    "for i in range(num_reviews):\n",
    "    if ((i+1) % 1000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1, num_reviews ))\n",
    "    clean_test_data_reviews.append( review_to_words( test_data[\"review\"][i] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Construct the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, to do the sentiment classification task, we want to first use word embedding to generate the representation of each word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing the sentences from the labeled training set....\n",
      "Parsing the sentences from the unlabeled training set....\n",
      "Time taken for getting all the text:  0.06516218185424805  seconds\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print('Parsing the sentences from the labeled training set....')\n",
    "start = time.time()\n",
    "\n",
    "for review in clean_labeled_train_reviews:\n",
    "    sentences.append(review)\n",
    "    \n",
    "print('Parsing the sentences from the unlabeled training set....')\n",
    "\n",
    "for review in clean_unlabeled_train_reviews:\n",
    "    sentences.append(review)\n",
    "    \n",
    "end = time.time()\n",
    "duration = end - start\n",
    "\n",
    "print(\"Time taken for getting all the text: \", duration, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have got all the words in the labeled and unlabeled dataset, we could use the word2vec model in [gensim](https://radimrehurek.com/gensim/models/word2vec.html) to generate the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-10 16:59:34,479 : INFO : collecting all words and their counts\n",
      "2018-03-10 16:59:34,482 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-10 16:59:35,044 : INFO : PROGRESS: at sentence #10000, processed 2385575 words, keeping 51527 word types\n",
      "2018-03-10 16:59:35,577 : INFO : PROGRESS: at sentence #20000, processed 4747504 words, keeping 67813 word types\n",
      "2018-03-10 16:59:36,114 : INFO : PROGRESS: at sentence #30000, processed 7109778 words, keeping 81728 word types\n",
      "2018-03-10 16:59:36,657 : INFO : PROGRESS: at sentence #40000, processed 9467206 words, keeping 92655 word types\n",
      "2018-03-10 16:59:37,236 : INFO : PROGRESS: at sentence #50000, processed 11825236 words, keeping 99583 word types\n",
      "2018-03-10 16:59:37,801 : INFO : PROGRESS: at sentence #60000, processed 14190901 words, keeping 105913 word types\n",
      "2018-03-10 16:59:38,365 : INFO : PROGRESS: at sentence #70000, processed 16573274 words, keeping 111433 word types\n",
      "2018-03-10 16:59:38,924 : INFO : PROGRESS: at sentence #80000, processed 18978383 words, keeping 116764 word types\n",
      "2018-03-10 16:59:39,483 : INFO : PROGRESS: at sentence #90000, processed 21360809 words, keeping 121461 word types\n",
      "2018-03-10 16:59:40,049 : INFO : PROGRESS: at sentence #100000, processed 23756442 words, keeping 123558 word types\n",
      "2018-03-10 16:59:40,601 : INFO : PROGRESS: at sentence #110000, processed 26138772 words, keeping 123567 word types\n",
      "2018-03-10 16:59:41,159 : INFO : PROGRESS: at sentence #120000, processed 28498204 words, keeping 123577 word types\n",
      "2018-03-10 16:59:41,722 : INFO : PROGRESS: at sentence #130000, processed 30857061 words, keeping 123587 word types\n",
      "2018-03-10 16:59:42,285 : INFO : PROGRESS: at sentence #140000, processed 33230318 words, keeping 123590 word types\n",
      "2018-03-10 16:59:42,342 : INFO : collected 123590 word types from a corpus of 33409920 raw words and 140764 sentences\n",
      "2018-03-10 16:59:42,343 : INFO : Loading a fresh vocabulary\n",
      "2018-03-10 16:59:42,478 : INFO : min_count=40 retains 23261 unique words (18% of original 123590, drops 100329)\n",
      "2018-03-10 16:59:42,480 : INFO : min_count=40 leaves 32733641 word corpus (97% of original 33409920, drops 676279)\n",
      "2018-03-10 16:59:42,577 : INFO : deleting the raw counts dictionary of 123590 items\n",
      "2018-03-10 16:59:42,584 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-03-10 16:59:42,585 : INFO : downsampling leaves estimated 24350254 word corpus (74.4% of prior 32733641)\n",
      "2018-03-10 16:59:42,587 : INFO : estimated required memory for 23261 words and 300 dimensions: 67456900 bytes\n",
      "2018-03-10 16:59:42,698 : INFO : resetting layer weights\n",
      "2018-03-10 16:59:43,113 : INFO : training model with 4 workers on 23261 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-03-10 16:59:44,131 : INFO : PROGRESS: at 0.45% examples, 544019 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 16:59:45,134 : INFO : PROGRESS: at 0.87% examples, 527814 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-10 16:59:46,145 : INFO : PROGRESS: at 1.33% examples, 540057 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 16:59:47,151 : INFO : PROGRESS: at 1.80% examples, 543649 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 16:59:48,169 : INFO : PROGRESS: at 2.28% examples, 550090 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-10 16:59:49,181 : INFO : PROGRESS: at 2.72% examples, 546633 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 16:59:50,196 : INFO : PROGRESS: at 3.15% examples, 542107 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 16:59:51,214 : INFO : PROGRESS: at 3.57% examples, 534722 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 16:59:52,219 : INFO : PROGRESS: at 4.05% examples, 539443 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 16:59:53,228 : INFO : PROGRESS: at 4.53% examples, 543728 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 16:59:54,239 : INFO : PROGRESS: at 4.99% examples, 543925 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 16:59:55,257 : INFO : PROGRESS: at 5.48% examples, 546772 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 16:59:56,262 : INFO : PROGRESS: at 5.96% examples, 549662 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 16:59:57,274 : INFO : PROGRESS: at 6.42% examples, 548817 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 16:59:58,290 : INFO : PROGRESS: at 6.85% examples, 547089 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 16:59:59,294 : INFO : PROGRESS: at 7.28% examples, 545060 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:00,303 : INFO : PROGRESS: at 7.70% examples, 543059 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:01,327 : INFO : PROGRESS: at 8.17% examples, 543571 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:02,330 : INFO : PROGRESS: at 8.62% examples, 544537 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:03,357 : INFO : PROGRESS: at 9.11% examples, 545575 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:04,366 : INFO : PROGRESS: at 9.53% examples, 543972 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:05,374 : INFO : PROGRESS: at 9.95% examples, 542405 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:06,392 : INFO : PROGRESS: at 10.33% examples, 538922 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:07,393 : INFO : PROGRESS: at 10.77% examples, 539152 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:08,419 : INFO : PROGRESS: at 11.22% examples, 539079 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:09,420 : INFO : PROGRESS: at 11.58% examples, 535622 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:10,435 : INFO : PROGRESS: at 11.96% examples, 532562 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:11,446 : INFO : PROGRESS: at 12.26% examples, 526604 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-10 17:00:12,469 : INFO : PROGRESS: at 12.62% examples, 522965 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:13,473 : INFO : PROGRESS: at 12.98% examples, 520773 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:14,476 : INFO : PROGRESS: at 13.35% examples, 518488 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:15,483 : INFO : PROGRESS: at 13.77% examples, 518087 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:16,484 : INFO : PROGRESS: at 14.17% examples, 517498 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:17,509 : INFO : PROGRESS: at 14.58% examples, 516464 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:18,537 : INFO : PROGRESS: at 15.00% examples, 516060 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:19,538 : INFO : PROGRESS: at 15.42% examples, 515861 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:20,539 : INFO : PROGRESS: at 15.83% examples, 515492 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:21,542 : INFO : PROGRESS: at 16.21% examples, 514162 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:22,550 : INFO : PROGRESS: at 16.62% examples, 513284 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:23,571 : INFO : PROGRESS: at 17.04% examples, 513110 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:24,578 : INFO : PROGRESS: at 17.45% examples, 512615 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:25,598 : INFO : PROGRESS: at 17.88% examples, 512281 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:26,602 : INFO : PROGRESS: at 18.30% examples, 512309 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:27,608 : INFO : PROGRESS: at 18.72% examples, 512334 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:28,635 : INFO : PROGRESS: at 19.12% examples, 511495 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:29,641 : INFO : PROGRESS: at 19.50% examples, 510409 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:30,646 : INFO : PROGRESS: at 19.91% examples, 509988 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:31,647 : INFO : PROGRESS: at 20.27% examples, 508488 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:32,674 : INFO : PROGRESS: at 20.67% examples, 507920 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:33,687 : INFO : PROGRESS: at 21.01% examples, 506094 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:34,687 : INFO : PROGRESS: at 21.38% examples, 505010 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:35,718 : INFO : PROGRESS: at 21.80% examples, 504748 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:36,729 : INFO : PROGRESS: at 22.20% examples, 504296 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:37,737 : INFO : PROGRESS: at 22.60% examples, 503857 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:38,742 : INFO : PROGRESS: at 22.99% examples, 503239 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-10 17:00:39,762 : INFO : PROGRESS: at 23.39% examples, 502530 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:40,777 : INFO : PROGRESS: at 23.77% examples, 501627 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:41,785 : INFO : PROGRESS: at 24.13% examples, 500436 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:42,811 : INFO : PROGRESS: at 24.48% examples, 498923 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:43,822 : INFO : PROGRESS: at 24.82% examples, 497458 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:44,826 : INFO : PROGRESS: at 25.14% examples, 495634 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:45,838 : INFO : PROGRESS: at 25.54% examples, 495300 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:46,843 : INFO : PROGRESS: at 25.92% examples, 494678 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:47,865 : INFO : PROGRESS: at 26.33% examples, 494404 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:48,884 : INFO : PROGRESS: at 26.76% examples, 494703 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:49,887 : INFO : PROGRESS: at 27.18% examples, 495113 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:50,894 : INFO : PROGRESS: at 27.62% examples, 495573 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:51,917 : INFO : PROGRESS: at 28.06% examples, 495790 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:00:52,933 : INFO : PROGRESS: at 28.49% examples, 496072 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-10 17:00:53,936 : INFO : PROGRESS: at 28.91% examples, 496409 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:54,938 : INFO : PROGRESS: at 29.35% examples, 496871 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:55,949 : INFO : PROGRESS: at 29.78% examples, 497236 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:56,950 : INFO : PROGRESS: at 30.20% examples, 497538 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:57,960 : INFO : PROGRESS: at 30.63% examples, 497910 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:58,970 : INFO : PROGRESS: at 31.07% examples, 498373 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:00:59,981 : INFO : PROGRESS: at 31.49% examples, 498768 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:00,987 : INFO : PROGRESS: at 31.92% examples, 498961 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:01,994 : INFO : PROGRESS: at 32.36% examples, 499381 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:02,997 : INFO : PROGRESS: at 32.79% examples, 499653 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:04,004 : INFO : PROGRESS: at 33.22% examples, 499982 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:05,017 : INFO : PROGRESS: at 33.66% examples, 500382 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:06,020 : INFO : PROGRESS: at 34.09% examples, 500788 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:07,020 : INFO : PROGRESS: at 34.52% examples, 501125 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:08,031 : INFO : PROGRESS: at 34.97% examples, 501506 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:09,039 : INFO : PROGRESS: at 35.40% examples, 501801 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:10,045 : INFO : PROGRESS: at 35.83% examples, 502103 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:11,051 : INFO : PROGRESS: at 36.28% examples, 502472 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:12,066 : INFO : PROGRESS: at 36.73% examples, 502746 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:13,078 : INFO : PROGRESS: at 37.16% examples, 502996 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:14,098 : INFO : PROGRESS: at 37.58% examples, 502820 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:15,105 : INFO : PROGRESS: at 37.98% examples, 502594 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:16,117 : INFO : PROGRESS: at 38.40% examples, 502760 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:17,120 : INFO : PROGRESS: at 38.83% examples, 502877 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:18,125 : INFO : PROGRESS: at 39.26% examples, 503068 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:19,134 : INFO : PROGRESS: at 39.70% examples, 503355 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:20,149 : INFO : PROGRESS: at 40.14% examples, 503632 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:21,151 : INFO : PROGRESS: at 40.57% examples, 503896 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:22,153 : INFO : PROGRESS: at 41.00% examples, 504072 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:23,166 : INFO : PROGRESS: at 41.43% examples, 504258 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:24,180 : INFO : PROGRESS: at 41.87% examples, 504493 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:25,193 : INFO : PROGRESS: at 42.31% examples, 504668 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:26,200 : INFO : PROGRESS: at 42.75% examples, 504929 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:27,207 : INFO : PROGRESS: at 43.19% examples, 505124 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-10 17:01:28,215 : INFO : PROGRESS: at 43.65% examples, 505445 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:29,218 : INFO : PROGRESS: at 44.08% examples, 505659 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:30,229 : INFO : PROGRESS: at 44.52% examples, 505902 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:31,241 : INFO : PROGRESS: at 44.97% examples, 506143 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:32,255 : INFO : PROGRESS: at 45.42% examples, 506424 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:33,264 : INFO : PROGRESS: at 45.87% examples, 506665 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:34,267 : INFO : PROGRESS: at 46.31% examples, 506864 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:35,267 : INFO : PROGRESS: at 46.74% examples, 507017 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-10 17:01:36,282 : INFO : PROGRESS: at 47.18% examples, 507289 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:37,289 : INFO : PROGRESS: at 47.63% examples, 507514 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:38,292 : INFO : PROGRESS: at 48.07% examples, 507752 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:39,297 : INFO : PROGRESS: at 48.49% examples, 507792 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:40,298 : INFO : PROGRESS: at 48.93% examples, 507973 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:41,301 : INFO : PROGRESS: at 49.37% examples, 508210 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:42,310 : INFO : PROGRESS: at 49.81% examples, 508409 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:43,314 : INFO : PROGRESS: at 50.24% examples, 508609 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:44,334 : INFO : PROGRESS: at 50.69% examples, 508826 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:45,343 : INFO : PROGRESS: at 51.13% examples, 509075 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:46,349 : INFO : PROGRESS: at 51.55% examples, 509253 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:47,358 : INFO : PROGRESS: at 52.00% examples, 509454 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:48,391 : INFO : PROGRESS: at 52.44% examples, 509584 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:49,392 : INFO : PROGRESS: at 52.88% examples, 509800 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:50,396 : INFO : PROGRESS: at 53.32% examples, 509997 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:51,401 : INFO : PROGRESS: at 53.76% examples, 510195 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:52,406 : INFO : PROGRESS: at 54.19% examples, 510367 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:01:53,423 : INFO : PROGRESS: at 54.64% examples, 510567 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:54,428 : INFO : PROGRESS: at 55.07% examples, 510698 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:55,437 : INFO : PROGRESS: at 55.51% examples, 510871 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:56,442 : INFO : PROGRESS: at 55.95% examples, 511062 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:57,448 : INFO : PROGRESS: at 56.39% examples, 511123 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:58,459 : INFO : PROGRESS: at 56.84% examples, 511313 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:01:59,461 : INFO : PROGRESS: at 57.28% examples, 511513 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-10 17:02:00,464 : INFO : PROGRESS: at 57.72% examples, 511607 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:01,470 : INFO : PROGRESS: at 58.16% examples, 511761 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:02,479 : INFO : PROGRESS: at 58.60% examples, 511911 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:03,486 : INFO : PROGRESS: at 59.04% examples, 512071 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:04,499 : INFO : PROGRESS: at 59.49% examples, 512240 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:05,505 : INFO : PROGRESS: at 59.93% examples, 512387 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:06,509 : INFO : PROGRESS: at 60.37% examples, 512552 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:07,518 : INFO : PROGRESS: at 60.80% examples, 512683 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:08,543 : INFO : PROGRESS: at 61.25% examples, 512805 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:09,564 : INFO : PROGRESS: at 61.69% examples, 512891 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:10,577 : INFO : PROGRESS: at 62.13% examples, 513042 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:11,588 : INFO : PROGRESS: at 62.58% examples, 513155 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:12,594 : INFO : PROGRESS: at 63.02% examples, 513293 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:13,599 : INFO : PROGRESS: at 63.47% examples, 513425 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:14,616 : INFO : PROGRESS: at 63.92% examples, 513523 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:15,619 : INFO : PROGRESS: at 64.36% examples, 513668 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:16,632 : INFO : PROGRESS: at 64.80% examples, 513781 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:17,643 : INFO : PROGRESS: at 65.24% examples, 513856 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:18,667 : INFO : PROGRESS: at 65.68% examples, 513884 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:19,668 : INFO : PROGRESS: at 66.12% examples, 513989 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:20,686 : INFO : PROGRESS: at 66.58% examples, 514123 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:21,694 : INFO : PROGRESS: at 67.01% examples, 514250 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:22,703 : INFO : PROGRESS: at 67.45% examples, 514373 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:23,707 : INFO : PROGRESS: at 67.89% examples, 514401 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:24,714 : INFO : PROGRESS: at 68.33% examples, 514517 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:25,723 : INFO : PROGRESS: at 68.76% examples, 514618 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:26,729 : INFO : PROGRESS: at 69.21% examples, 514741 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:27,737 : INFO : PROGRESS: at 69.65% examples, 514852 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:28,755 : INFO : PROGRESS: at 70.09% examples, 514965 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:29,768 : INFO : PROGRESS: at 70.52% examples, 515045 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:30,786 : INFO : PROGRESS: at 70.98% examples, 515178 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:31,787 : INFO : PROGRESS: at 71.39% examples, 515249 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:32,795 : INFO : PROGRESS: at 71.82% examples, 515265 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:33,796 : INFO : PROGRESS: at 72.26% examples, 515384 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:34,803 : INFO : PROGRESS: at 72.70% examples, 515495 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:35,822 : INFO : PROGRESS: at 73.14% examples, 515602 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:36,839 : INFO : PROGRESS: at 73.59% examples, 515725 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:37,854 : INFO : PROGRESS: at 74.02% examples, 515789 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:38,855 : INFO : PROGRESS: at 74.46% examples, 515904 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:39,858 : INFO : PROGRESS: at 74.89% examples, 515984 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:40,859 : INFO : PROGRESS: at 75.32% examples, 516015 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:41,865 : INFO : PROGRESS: at 75.75% examples, 516051 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:42,872 : INFO : PROGRESS: at 76.20% examples, 516155 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:43,891 : INFO : PROGRESS: at 76.65% examples, 516234 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:44,896 : INFO : PROGRESS: at 77.07% examples, 516260 words/s, in_qsize 5, out_qsize 2\n",
      "2018-03-10 17:02:45,907 : INFO : PROGRESS: at 77.53% examples, 516360 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:46,922 : INFO : PROGRESS: at 77.98% examples, 516466 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:47,936 : INFO : PROGRESS: at 78.41% examples, 516508 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:48,942 : INFO : PROGRESS: at 78.83% examples, 516485 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:49,957 : INFO : PROGRESS: at 79.28% examples, 516593 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:50,965 : INFO : PROGRESS: at 79.72% examples, 516671 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:51,979 : INFO : PROGRESS: at 80.17% examples, 516776 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:52,985 : INFO : PROGRESS: at 80.59% examples, 516801 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:53,986 : INFO : PROGRESS: at 81.03% examples, 516902 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:55,001 : INFO : PROGRESS: at 81.47% examples, 516925 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:56,006 : INFO : PROGRESS: at 81.90% examples, 517002 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:57,012 : INFO : PROGRESS: at 82.34% examples, 517052 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:02:58,030 : INFO : PROGRESS: at 82.79% examples, 517097 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:02:59,052 : INFO : PROGRESS: at 83.24% examples, 517177 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:00,053 : INFO : PROGRESS: at 83.69% examples, 517273 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:01,055 : INFO : PROGRESS: at 84.12% examples, 517329 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:02,070 : INFO : PROGRESS: at 84.56% examples, 517358 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:03,079 : INFO : PROGRESS: at 84.99% examples, 517364 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:04,097 : INFO : PROGRESS: at 85.44% examples, 517382 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:05,136 : INFO : PROGRESS: at 85.86% examples, 517279 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:06,142 : INFO : PROGRESS: at 86.16% examples, 516481 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:07,169 : INFO : PROGRESS: at 86.48% examples, 515740 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:08,178 : INFO : PROGRESS: at 86.88% examples, 515655 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:09,184 : INFO : PROGRESS: at 87.28% examples, 515469 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:10,195 : INFO : PROGRESS: at 87.67% examples, 515272 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:11,196 : INFO : PROGRESS: at 88.06% examples, 515030 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:12,220 : INFO : PROGRESS: at 88.44% examples, 514697 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:13,247 : INFO : PROGRESS: at 88.82% examples, 514457 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:14,270 : INFO : PROGRESS: at 89.18% examples, 513968 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:15,289 : INFO : PROGRESS: at 89.55% examples, 513661 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:16,297 : INFO : PROGRESS: at 89.91% examples, 513275 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:17,317 : INFO : PROGRESS: at 90.24% examples, 512757 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:18,326 : INFO : PROGRESS: at 90.57% examples, 512253 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:19,342 : INFO : PROGRESS: at 90.99% examples, 512170 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:20,355 : INFO : PROGRESS: at 91.38% examples, 512047 words/s, in_qsize 8, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-10 17:03:21,357 : INFO : PROGRESS: at 91.72% examples, 511626 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:22,358 : INFO : PROGRESS: at 92.13% examples, 511543 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:23,382 : INFO : PROGRESS: at 92.55% examples, 511505 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:24,402 : INFO : PROGRESS: at 92.93% examples, 511286 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:25,420 : INFO : PROGRESS: at 93.33% examples, 511102 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-10 17:03:26,430 : INFO : PROGRESS: at 93.74% examples, 511099 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:27,435 : INFO : PROGRESS: at 94.12% examples, 510880 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:28,446 : INFO : PROGRESS: at 94.45% examples, 510394 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:29,453 : INFO : PROGRESS: at 94.88% examples, 510410 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:30,462 : INFO : PROGRESS: at 95.26% examples, 510189 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:31,468 : INFO : PROGRESS: at 95.61% examples, 509832 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:32,498 : INFO : PROGRESS: at 95.95% examples, 509358 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:33,518 : INFO : PROGRESS: at 96.37% examples, 509245 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:34,525 : INFO : PROGRESS: at 96.70% examples, 508757 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-10 17:03:35,530 : INFO : PROGRESS: at 97.00% examples, 508161 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:36,559 : INFO : PROGRESS: at 97.33% examples, 507620 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:37,561 : INFO : PROGRESS: at 97.70% examples, 507321 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:38,595 : INFO : PROGRESS: at 98.01% examples, 506731 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:39,596 : INFO : PROGRESS: at 98.28% examples, 506011 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:40,631 : INFO : PROGRESS: at 98.63% examples, 505587 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:41,632 : INFO : PROGRESS: at 98.97% examples, 505174 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:42,683 : INFO : PROGRESS: at 99.34% examples, 504842 words/s, in_qsize 8, out_qsize 2\n",
      "2018-03-10 17:03:43,720 : INFO : PROGRESS: at 99.65% examples, 504239 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-10 17:03:44,724 : INFO : PROGRESS: at 99.94% examples, 503595 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-10 17:03:44,843 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-10 17:03:44,856 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-10 17:03:44,866 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-10 17:03:44,868 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-10 17:03:44,870 : INFO : training on 167049600 raw words (121747186 effective words) took 241.8s, 503601 effective words/s\n",
      "2018-03-10 17:03:44,873 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-03-10 17:03:45,471 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-03-10 17:03:45,475 : INFO : not storing attribute syn0norm\n",
      "2018-03-10 17:03:45,478 : INFO : not storing attribute cum_table\n",
      "2018-03-10 17:03:47,239 : INFO : saved 300features_40minwords_10context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for training the word vectors:  252.76197171211243  seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "print(\"Training model...\")\n",
    "start = time.time()\n",
    "model = Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "\n",
    "print(\"Total time for training the word vectors: \", duration, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got the word vector model. To load this model, you could use the following code:\n",
    "\n",
    "```Python\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(sentence, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((300,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    #\n",
    "    # Loop over each word in the review\n",
    "    for word in sentence: \n",
    "        nwords = nwords + 1.\n",
    "        featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "        if counter % 1000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for training reviews\n",
      "Review 0 of 25000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'wiz' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-81e1aed4a0f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating average feature vecs for training reviews\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_labeled_train_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating average feature vecs for test reviews\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-08d8541a180a>\u001b[0m in \u001b[0;36mgetAvgFeatureVecs\u001b[1;34m(reviews, model, num_features)\u001b[0m\n\u001b[0;32m     17\u001b[0m        \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m        \u001b[1;31m# Call the function (defined above) that makes average feature vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mreviewFeatureVecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmakeFeatureVec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m        \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m        \u001b[1;31m# Increment the counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-c2149b59a2ef>\u001b[0m in \u001b[0;36mmakeFeatureVec\u001b[1;34m(sentence, model, num_features)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mnwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnwords\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mfeatureVec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureVec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Divide the result by the number of words to get the average\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m   1279\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[1;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m         \"\"\"\n\u001b[1;32m-> 1281\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'wiz' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "print(\"Creating average feature vecs for training reviews\")\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_labeled_train_reviews, model, 300)\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'all',\n",
       " 'this',\n",
       " 'stuff',\n",
       " 'going',\n",
       " 'down',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'with',\n",
       " 'mj',\n",
       " 'i',\n",
       " 've',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'to',\n",
       " 'his',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'the',\n",
       " 'odd',\n",
       " 'documentary',\n",
       " 'here',\n",
       " 'and',\n",
       " 'there',\n",
       " 'watched',\n",
       " 'the',\n",
       " 'wiz',\n",
       " 'and',\n",
       " 'watched',\n",
       " 'moonwalker',\n",
       " 'again',\n",
       " 'maybe',\n",
       " 'i',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'certain',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'this',\n",
       " 'guy',\n",
       " 'who',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'was',\n",
       " 'really',\n",
       " 'cool',\n",
       " 'in',\n",
       " 'the',\n",
       " 'eighties',\n",
       " 'just',\n",
       " 'to',\n",
       " 'maybe',\n",
       " 'make',\n",
       " 'up',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'whether',\n",
       " 'he',\n",
       " 'is',\n",
       " 'guilty',\n",
       " 'or',\n",
       " 'innocent',\n",
       " 'moonwalker',\n",
       " 'is',\n",
       " 'part',\n",
       " 'biography',\n",
       " 'part',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'which',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'going',\n",
       " 'to',\n",
       " 'see',\n",
       " 'at',\n",
       " 'the',\n",
       " 'cinema',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'originally',\n",
       " 'released',\n",
       " 'some',\n",
       " 'of',\n",
       " 'it',\n",
       " 'has',\n",
       " 'subtle',\n",
       " 'messages',\n",
       " 'about',\n",
       " 'mj',\n",
       " 's',\n",
       " 'feeling',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'press',\n",
       " 'and',\n",
       " 'also',\n",
       " 'the',\n",
       " 'obvious',\n",
       " 'message',\n",
       " 'of',\n",
       " 'drugs',\n",
       " 'are',\n",
       " 'bad',\n",
       " 'm',\n",
       " 'kay',\n",
       " 'visually',\n",
       " 'impressive',\n",
       " 'but',\n",
       " 'of',\n",
       " 'course',\n",
       " 'this',\n",
       " 'is',\n",
       " 'all',\n",
       " 'about',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'so',\n",
       " 'unless',\n",
       " 'you',\n",
       " 'remotely',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'in',\n",
       " 'anyway',\n",
       " 'then',\n",
       " 'you',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'hate',\n",
       " 'this',\n",
       " 'and',\n",
       " 'find',\n",
       " 'it',\n",
       " 'boring',\n",
       " 'some',\n",
       " 'may',\n",
       " 'call',\n",
       " 'mj',\n",
       " 'an',\n",
       " 'egotist',\n",
       " 'for',\n",
       " 'consenting',\n",
       " 'to',\n",
       " 'the',\n",
       " 'making',\n",
       " 'of',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'mj',\n",
       " 'and',\n",
       " 'most',\n",
       " 'of',\n",
       " 'his',\n",
       " 'fans',\n",
       " 'would',\n",
       " 'say',\n",
       " 'that',\n",
       " 'he',\n",
       " 'made',\n",
       " 'it',\n",
       " 'for',\n",
       " 'the',\n",
       " 'fans',\n",
       " 'which',\n",
       " 'if',\n",
       " 'true',\n",
       " 'is',\n",
       " 'really',\n",
       " 'nice',\n",
       " 'of',\n",
       " 'him',\n",
       " 'the',\n",
       " 'actual',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'bit',\n",
       " 'when',\n",
       " 'it',\n",
       " 'finally',\n",
       " 'starts',\n",
       " 'is',\n",
       " 'only',\n",
       " 'on',\n",
       " 'for',\n",
       " 'minutes',\n",
       " 'or',\n",
       " 'so',\n",
       " 'excluding',\n",
       " 'the',\n",
       " 'smooth',\n",
       " 'criminal',\n",
       " 'sequence',\n",
       " 'and',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'is',\n",
       " 'convincing',\n",
       " 'as',\n",
       " 'a',\n",
       " 'psychopathic',\n",
       " 'all',\n",
       " 'powerful',\n",
       " 'drug',\n",
       " 'lord',\n",
       " 'why',\n",
       " 'he',\n",
       " 'wants',\n",
       " 'mj',\n",
       " 'dead',\n",
       " 'so',\n",
       " 'bad',\n",
       " 'is',\n",
       " 'beyond',\n",
       " 'me',\n",
       " 'because',\n",
       " 'mj',\n",
       " 'overheard',\n",
       " 'his',\n",
       " 'plans',\n",
       " 'nah',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 's',\n",
       " 'character',\n",
       " 'ranted',\n",
       " 'that',\n",
       " 'he',\n",
       " 'wanted',\n",
       " 'people',\n",
       " 'to',\n",
       " 'know',\n",
       " 'it',\n",
       " 'is',\n",
       " 'he',\n",
       " 'who',\n",
       " 'is',\n",
       " 'supplying',\n",
       " 'drugs',\n",
       " 'etc',\n",
       " 'so',\n",
       " 'i',\n",
       " 'dunno',\n",
       " 'maybe',\n",
       " 'he',\n",
       " 'just',\n",
       " 'hates',\n",
       " 'mj',\n",
       " 's',\n",
       " 'music',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'cool',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'turning',\n",
       " 'into',\n",
       " 'a',\n",
       " 'car',\n",
       " 'and',\n",
       " 'a',\n",
       " 'robot',\n",
       " 'and',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'speed',\n",
       " 'demon',\n",
       " 'sequence',\n",
       " 'also',\n",
       " 'the',\n",
       " 'director',\n",
       " 'must',\n",
       " 'have',\n",
       " 'had',\n",
       " 'the',\n",
       " 'patience',\n",
       " 'of',\n",
       " 'a',\n",
       " 'saint',\n",
       " 'when',\n",
       " 'it',\n",
       " 'came',\n",
       " 'to',\n",
       " 'filming',\n",
       " 'the',\n",
       " 'kiddy',\n",
       " 'bad',\n",
       " 'sequence',\n",
       " 'as',\n",
       " 'usually',\n",
       " 'directors',\n",
       " 'hate',\n",
       " 'working',\n",
       " 'with',\n",
       " 'one',\n",
       " 'kid',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'a',\n",
       " 'whole',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'them',\n",
       " 'performing',\n",
       " 'a',\n",
       " 'complex',\n",
       " 'dance',\n",
       " 'scene',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'for',\n",
       " 'people',\n",
       " 'who',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'on',\n",
       " 'one',\n",
       " 'level',\n",
       " 'or',\n",
       " 'another',\n",
       " 'which',\n",
       " 'i',\n",
       " 'think',\n",
       " 'is',\n",
       " 'most',\n",
       " 'people',\n",
       " 'if',\n",
       " 'not',\n",
       " 'then',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'it',\n",
       " 'does',\n",
       " 'try',\n",
       " 'and',\n",
       " 'give',\n",
       " 'off',\n",
       " 'a',\n",
       " 'wholesome',\n",
       " 'message',\n",
       " 'and',\n",
       " 'ironically',\n",
       " 'mj',\n",
       " 's',\n",
       " 'bestest',\n",
       " 'buddy',\n",
       " 'in',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'a',\n",
       " 'girl',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'is',\n",
       " 'truly',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'talented',\n",
       " 'people',\n",
       " 'ever',\n",
       " 'to',\n",
       " 'grace',\n",
       " 'this',\n",
       " 'planet',\n",
       " 'but',\n",
       " 'is',\n",
       " 'he',\n",
       " 'guilty',\n",
       " 'well',\n",
       " 'with',\n",
       " 'all',\n",
       " 'the',\n",
       " 'attention',\n",
       " 'i',\n",
       " 've',\n",
       " 'gave',\n",
       " 'this',\n",
       " 'subject',\n",
       " 'hmmm',\n",
       " 'well',\n",
       " 'i',\n",
       " 'don',\n",
       " 't',\n",
       " 'know',\n",
       " 'because',\n",
       " 'people',\n",
       " 'can',\n",
       " 'be',\n",
       " 'different',\n",
       " 'behind',\n",
       " 'closed',\n",
       " 'doors',\n",
       " 'i',\n",
       " 'know',\n",
       " 'this',\n",
       " 'for',\n",
       " 'a',\n",
       " 'fact',\n",
       " 'he',\n",
       " 'is',\n",
       " 'either',\n",
       " 'an',\n",
       " 'extremely',\n",
       " 'nice',\n",
       " 'but',\n",
       " 'stupid',\n",
       " 'guy',\n",
       " 'or',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'sickest',\n",
       " 'liars',\n",
       " 'i',\n",
       " 'hope',\n",
       " 'he',\n",
       " 'is',\n",
       " 'not',\n",
       " 'the',\n",
       " 'latter']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_labeled_train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
