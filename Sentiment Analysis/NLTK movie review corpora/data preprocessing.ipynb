{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Text data preparation is very important in this sentiment analysis project. In this section, firstly, we are going to load all the modules we need in this analysis and introduce the NLTK movie reviews corpora. Secondly, we store all the data in the python list. Thirdly, we briefly talk about how to erase the punctuation, contraction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modules Preparation & Movie Reviews Corpora\n",
    "\n",
    "The Python modules we are going to use are listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Import the data set:movie reviews\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import ClassifierI\n",
    "\n",
    "from sklearn import grid_search\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report,confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "from sklearn.preprocessing import scale\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from statistics import mode\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie reviews corpora in NLTK contains 2000 movie reviews and each movie review is stored in a text file. If you want to see the raw data directly in your PC, just type **appdata** in the path and go to the file **nltk_data**. Then choose the corpora and after opening the movie_reviews file, you can see the raw text data. \n",
    "\n",
    "In this corpora, you could see half of the reviews are positive and the second half are negative. You can also get the details of this corpora just by running the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the text file names by using the fileids method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos/cv000_29590.txt', 'pos/cv001_18431.txt', 'pos/cv002_15918.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids('pos')[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for instance, if we want to get access to all the words in a text file by a file name, use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words(movie_reviews.fileids('pos')[movie_reviews.fileids('pos').index('pos/cv000_29590.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Input the Data to Python\n",
    "\n",
    "After knowing these above methods, we can put these files in a document. One thing to remember is that we should random shuffle the documents to erase the bias in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((movie_reviews.words(fileid), category))\n",
    "        \n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since we have got all the words, we need to create the features for the following analysis. The features we are going to use here are the most frequent words used in the movie reviews(We would use new features later to see the change in the accuracy of the classifiers). Here, the method **FreqDist** is used to list the words with their frequencies so that we can pick, for instance, first 3000 of them and use these words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())  # convert all the words to lowercase\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# Use the top 3000 words as the keys\n",
    "word_features = list(all_words.keys())[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have created the documents consist of all the movie review files and the word features. So the next thing is to judge whether a specific file contains featured words. An option is to create a function and decide if a word in word feature list also exists in a movie review text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features(text):\n",
    "    words = set(text)\n",
    "    featuresets = {}\n",
    "    for w in word_features:\n",
    "        featuresets[w] = (w in words)\n",
    "        \n",
    "    return featuresets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could use this function to analyze each text file in the documents and at last create the trainning set, the validation set(a data set which is always being used to compare the performance of different models) and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev),category) for (rev, category) in documents]\n",
    "\n",
    "training_set = featuresets[:1800]\n",
    "validation_set = featuresets[1800:1900]\n",
    "testing_set = featuresets[1900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we use the first 5000 most frequent words as word features. However, this may lead to incorrect results because many nouns such as **movie** does not imply a viewer's attitude towards a movie. A more reliable approach is to consider the adjectives and adverbs as features because these words are more close to a viewer's opinion. To reach this goal, we need to use the **pos_tag** function to tag a word's part of speech and select the adjectives and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())  # convert all the words to lowercase\n",
    "\n",
    "pos = pos_tag(all_words)\n",
    "\n",
    "adj_adv = []\n",
    "\n",
    "for w in pos:\n",
    "    if w[1][0] == 'J' or w[1][0] == 'R':\n",
    "        adj_adv.append(w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we are about to choose the most frequent 5000 adjectives and adverbs as word features to continue the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj_adv = nltk.FreqDist(adj_adv)\n",
    "\n",
    "# Use the top 5000 words as the keys\n",
    "word_features = list(adj_adv.keys())[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gaining the word_features, we construct new training set and testing set again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev),category) for (rev, category) in documents]\n",
    "\n",
    "training_set = featuresets[:1800]\n",
    "validation_set = featuresets[1800:1900]\n",
    "testing_set = featuresets[1900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clean the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want our model to achieve better results in doing classification, we should clean the text data first. In general, we could improve the quality of our text by doing the following things:\n",
    "\n",
    "1. eliminate stopwords\n",
    "2. eliminate punctuation\n",
    "3. contraction (He's -> He is)\n",
    "4. conversion into lowercase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Eliminate the stopwords\n",
    "\n",
    "Stopwords like 'to' are meaningless in natural language processing. Hence, we can delete them first to get a much cleaner dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesitate', 'ask', 'questions']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "words=[\"Don't\", 'hesitate','to','ask','questions']\n",
    "[word for word in words if word not in stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK is marvelous because it contains many languages' stopwords. They can be directly used to help with our following analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the proportion of stopwords in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\",\"I am very happy\"]\n",
    "words = []\n",
    "for sentence in text:\n",
    "    for word in word_tokenize(sentence):\n",
    "        words.append(word)\n",
    "\n",
    "lower_words = [word.lower() for word in words]\n",
    "        \n",
    "text_f = [word for word in lower_words if word in stops]\n",
    "\n",
    "proportion = len(text_f)/len(lower_words)\n",
    "print(proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Eliminate punctuations\n",
    "\n",
    "Punctuations are also useless in our analysis. To delete all the punctuations, we need to use the **re** module and regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "tokenized_docs = [word_tokenize(doc) for doc in text]\n",
    "# [] means character classes. Character classes provide a way to match only one of a specific set of characters\n",
    "x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = x.sub(u'',token) # In python, r\"XXX\" means normal string. u\"XXX\" means unicode\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Contraction\n",
    "\n",
    "In English, many contractions appear like she's, we're. We could also use regular expressions to convert them into their original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Jack. He is Jim. He will participate in our team! He have played football for ten years~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r\"won\\'t\", \"will not\"),\n",
    "    (r\"can\\'t\", \"cannot\"),\n",
    "    (r\"I\\'m\",\"I am\"),\n",
    "    (r\"ain\\'t\", 'is not'),\n",
    "    # \\g<1> are using back-references to capture part of the matched pattern\n",
    "    # \\g means referencing group content in the previous pattern. <1> means the first group. In the following case, the first group is w+\n",
    "    (r\"(\\w+)\\'ll\",\"\\g<1> will\"),\n",
    "    (r\"(\\w+)n\\'t\", \"\\g<1> not\"),\n",
    "    (r\"(\\w+)\\'ve\", \"\\g<1> have\"),\n",
    "    (r\"(\\w+)\\'s\", \"\\g<1> is\"),\n",
    "    (r\"(\\w+)\\'re\", \"\\g<1> are\"),\n",
    "    (r\"(\\w+)\\'d\", \"\\g<1> would\")\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self,patterns = replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex),repl) for (regex, repl) in replacement_patterns]\n",
    "        \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s, count) = re.subn(pattern, repl,s) # subn returns the times of replacement\n",
    "        return s\n",
    "    \n",
    "Object = RegexpReplacer()\n",
    "Object.replace(\"I'm Jack. He's Jim. He'll participate in our team! He've played football for ten years~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Conversion to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'a', 'pleasant', 'evening', '.', 'guests', ',', 'who', 'came', 'from', 'us', 'arrived', 'at', 'the', 'venue', 'food', 'was', 'tasty', '.', 'i', 'am', 'very', 'happy']\n"
     ]
    }
   ],
   "source": [
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\",\"I am very happy\"]\n",
    "words = []\n",
    "for sentence in text:\n",
    "    for word in word_tokenize(sentence):\n",
    "        words.append(word)\n",
    "\n",
    "lower_words = [word.lower() for word in words]\n",
    "print(lower_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
