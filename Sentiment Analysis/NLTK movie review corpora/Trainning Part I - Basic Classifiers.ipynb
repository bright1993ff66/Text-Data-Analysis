{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we are going to use multiple classifiers to split the positive and negative reviews. With scikit-learn, it is now very convenient to train the data with much more classifers and much less codes.\n",
    "\n",
    "For the classification tasks, we can use Naive Bayes, Logistics Regression, etc to train the data. Firstly, I will give a brief introduction about the priciples of these algorithms. Then I will apply these algorithms to the dataset so that we can see which one performs the best. At last, I am going to show you how to tune the hyperparameters in these classifiers to reach a good result. Moreover, for the type of features, I am going to use the most frequent 5000 words as featuresets. If you chose adjectives and adverbs, the accuracy rate would be higher. You could just use the codes below but don't forget to change the featuresets. \n",
    "\n",
    "In the Trainning Part II, I will use multilayer perception to do the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classifiers for Classification\n",
    "\n",
    "In this section, we are going to use Naive Bayes, Multinomial Naive Bayes, Bernoulli Classifier, Logistics Regression Classifier, SGD Classifier, SVC Classifier, Linear SVC Classifier, NuSVC classifier to do the classification. More information about these classifiers can be found here:[Classification - Supervised Learning](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, Naive Bayes actually represents a set of supervied learning algorithms based on applying Bayes' theorem with **the \"naive\" assumption of independence between every pair of features**. Given a class variable $y$ and a dependent feature vector $x_1$ through $x_n$, Bayes theorem states the following relationship:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    P(y|x_1...x_n) = \\frac{P(y)P(x_1..x_n|y)}{P(x_1...x_n)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above formular, we find calculating $P(x_1...x_n|y)$ is very difficult. But using the \"naive\" assumption, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_{n}) = P(x_i|y) \\quad i = 1...n\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then the relationship can be simplified to:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(y|x_1...x_n) = \\frac{P(y)\\prod_{i=1}^{n}P(x_i|y)}{P(x_1...x_n)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator and $P(y)$ are constants and we could get $y$ by maximizing the $\\prod_{i=1}^{n}P(x_i|y)$, which means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\ty = \\mathop{\\arg\\max}_{y}P(y)\\prod_{i=1}^{n}P(x_i|y) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier has many versions. The difference between these versions is that they have different methods to approximate $P(x_i|y)$. For the version that we explained above, if we want to use it to do the classification, just use the **NaiveBayesClassifier** in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algo accuracy percent:  73.0\n",
      "Most Informative Features\n",
      "                   dread = True              pos : neg    =      9.6 : 1.0\n",
      "                fairness = True              neg : pos    =      9.1 : 1.0\n",
      "                  hudson = True              neg : pos    =      9.1 : 1.0\n",
      "          excruciatingly = True              neg : pos    =      9.1 : 1.0\n",
      "               uplifting = True              pos : neg    =      8.5 : 1.0\n",
      "                 conveys = True              pos : neg    =      7.6 : 1.0\n",
      "                   terri = True              neg : pos    =      7.1 : 1.0\n",
      "                observes = True              pos : neg    =      6.9 : 1.0\n",
      "                 labeled = True              pos : neg    =      6.9 : 1.0\n",
      "                    coen = True              pos : neg    =      6.5 : 1.0\n",
      "              unbearable = True              neg : pos    =      6.3 : 1.0\n",
      "                    taxi = True              pos : neg    =      6.1 : 1.0\n",
      "                   anger = True              pos : neg    =      6.0 : 1.0\n",
      "             beautifully = True              pos : neg    =      6.0 : 1.0\n",
      "                   awful = True              neg : pos    =      5.9 : 1.0\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print('Naive Bayes Algo accuracy percent: ', (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another version: Gausian Naive Bayes implements Gaussian Naive Bayes algorithm for classification. This method approximate $P(x_i|y)$ by:\n",
    "\n",
    "\\begin{equation}\n",
    "P(x_i|y) = \\frac{1}{\\sqrt{2\\pi{\\sigma_y}^2}}exp(-\\frac{(x_i-{\\mu_y})^2}{2{\\sigma_y}^2})\n",
    "\\end{equation}\n",
    "\n",
    "The parameters $\\sigma_y$ and $\\mu_y$ are estimated by maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gaussian NB accuracy rate:  55.00000000000001 %\n",
      "Wall time: 663 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "training_review = []\n",
    "training_target = []\n",
    "test_review = []\n",
    "test_target = []\n",
    "\n",
    "for text in training_set:\n",
    "    training_review.append(list(text[0].values()))\n",
    "    training_target.append(text[1])\n",
    "    \n",
    "for text in testing_set:\n",
    "    test_review.append(list(text[0].values()))\n",
    "    test_target.append(text[1])\n",
    "    \n",
    "predictions = gnb.fit(training_review, training_target).predict(test_review)\n",
    "count = 0\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == test_target[i]:\n",
    "        count += 1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "print('The Gaussian NB accuracy rate: ', (count/len(predictions))*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the Multinomial Naive Bayes estimate the $P(x_i|y)$ by the following way: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\theta_{yi}} = \\frac{N_{yi} + \\alpha}{N_{y} + \\alpha n}\n",
    "\\end{equation}\n",
    "\n",
    "where $N_{yi}$ means the number of times feature i appears in a sample of class y in the training set. $N_y$ means the total number of all features for class y. For this movie review data set, we can also use multinomial NB classifier to train the data and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy rate:  78.0\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)                           \n",
    "print('MNB_classifier accuracy rate: ', (nltk.classify.accuracy(MNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last kind of Naive Bayes approach we are going to introduce is the Bernoulli Naive Bayes classifer, which is based on multivariate bernoulli distribution. There might be multiple features but each one is assumed to be **a binary-valued variable**. For this classifier, it estimates $P(x_i|y)$ by:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(x_i|y) = P(i|y)x_i + (1-P(i|y))(1-x_i)\n",
    "\\end{equation}\n",
    "\n",
    "i means feature. So the implementation of this method on movie review corpora is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli_classifier accuracy rate:  75.0\n"
     ]
    }
   ],
   "source": [
    "Bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "Bernoulli_classifier.train(training_set)                           \n",
    "print('Bernoulli_classifier accuracy rate: ', (nltk.classify.accuracy(Bernoulli_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Logistics Regression\n",
    "\n",
    "Logistics regression, also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier, is a linear model for classification. In this model, the probabilities describing the possible outcomes of a single trial are modeled using the following logistic function:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = \\frac{L}{1 + e^{-k(x-x_0)}}\n",
    "\\end{equation}\n",
    "\n",
    "where L means the curve's maximum value, $x_0$ means the x-value of the sigmoid's midpoint, and k means the stepness of the curve. More information about this function can be found here: [logistic function](https://en.wikipedia.org/wiki/Logistic_function)\n",
    "\n",
    "In general, we always use the following form of logistic function:\n",
    "\n",
    "\\begin{equation}\n",
    "    h_{\\theta}(x) = \\frac{L}{1 + e^{-\\theta x}}\n",
    "\\end{equation}\n",
    "\n",
    "The above function is also called sigmoid function, which is also widely used as activation function in neural network. \n",
    "\n",
    "The cost function for logistics regression is:\n",
    "\n",
    "\\begin{equation}\n",
    "    J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}[y_i logh_{\\theta}(x) + (1 - y_i)log(1-h_{\\theta}(x))]\n",
    "\\end{equation}\n",
    "\n",
    "Why do we choose this cost function? How to get this cost function?\n",
    "\n",
    "Firstly, because if we just calculat the estimation $h_{\\theta}(x)$ and plug it in the ordinary cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "    J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - h_{\\theta}(x))\n",
    "\\end{equation}\n",
    "\n",
    "$J(\\theta)$ is non-convex and we can't find the global minimal. Hence we decide to change the cost function. For the cost function except the $\\frac{1}{m}$ and summation, we could use the following trick to deal with predictions:\n",
    "\n",
    "\\begin{equation}\n",
    "cost=\n",
    "\\begin{cases}\n",
    "-log(h_{\\theta}(x))& \\text{y=1}\\\\\n",
    "-log(1-h_{\\theta}(x))& \\text{y=0}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Since y is a variable with only two possible values(y=1 and y=0), then the above thing actually equal to the cost function of logistics regression we have listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In scikit-learn, we can use the LogisticRegression() to construct the classifier. The classifier we constructed in this way can **fit binary**, **One-vs- Rest**, or **multinomial logistic regression** with **optional L2** or **L1** regularization. For this movie reviews dataset, if we use logistics regression, we can work out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_classifier accuracy rate:  73.0\n"
     ]
    }
   ],
   "source": [
    "logistic_classifier = SklearnClassifier(LogisticRegression())\n",
    "logistic_classifier.train(training_set)                           \n",
    "print('logistic_classifier accuracy rate: ', (nltk.classify.accuracy(logistic_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 SGD Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier implements linear classifier such as SVM and logistics regression together with stochastic gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD_classifier accuracy rate:  65.0\n"
     ]
    }
   ],
   "source": [
    "SGD_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGD_classifier.train(training_set)                           \n",
    "print('SGD_classifier accuracy rate: ', (nltk.classify.accuracy(SGD_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 SVM\n",
    "\n",
    "There are many forms of SVM classifiers in scikit-learn. In this post we mainly talk about SVC, NuSVC, LinearSVC. More detailed information about these classifiers can be found here: [SVM](http://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation)\n",
    "\n",
    "For this dataset, we could run these classifiers and see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC_Classifier accuracy rate:  45.0\n",
      "NuSVC_Classifier accuracy rate:  75.0\n",
      "LinearSVC_Classifier accuracy rate:  72.0\n"
     ]
    }
   ],
   "source": [
    "SVC_Classifier = SklearnClassifier(SVC())\n",
    "SVC_Classifier.train(training_set)\n",
    "print('SVC_Classifier accuracy rate: ', (nltk.classify.accuracy(SVC_Classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_Classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_Classifier.train(training_set)\n",
    "print('NuSVC_Classifier accuracy rate: ', (nltk.classify.accuracy(NuSVC_Classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_Classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_Classifier.train(training_set)\n",
    "print('LinearSVC_Classifier accuracy rate: ', (nltk.classify.accuracy(LinearSVC_Classifier, testing_set))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tune the Hyperparameters\n",
    "\n",
    "From the previous results, we have reached great results from various classifiers. Can we do better? The answer is definitely. For instance, the SVC classifier has many parameters. We can set the values of some of them to get a better results. Firstly, we can get a brief indea of the default values of this classifier:\n",
    "\n",
    "```Python\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',max_iter=-1, probability=False, random_state=None, shrinking=True,tol=0.001, verbose=False)\n",
    "```\n",
    "\n",
    "To tune the hyperparameters, we need to construct the following things:\n",
    "1. One estimator(for instance: sklearn.svm.NuSVC)\n",
    "2. One parameter space\n",
    "3. One method which can let you select the different combinations of parameter values\n",
    "4. A cross-validation technique\n",
    "5. One function to measure the performance of different combinations of parameter values\n",
    "\n",
    "In scikit-learn, we have mainly three methods for tuning hyperparameters, which are GridSearchCV, RandomizedSearchCV and bayes optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the a dictionary called parameters to list all the parameters we are interested in and their possible values. Then we use the GridSearchCV function to feed the parameters dictionary to the SVC_classifier. \n",
    "\n",
    "**Remember**: this may takes a quite a long time for a laptop to train the classifiers with different hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "parameters={'kernel':('rbf','sigmoid'),'C':[1,5,10]}\n",
    "SVC_classifier = SVC()\n",
    "clf = GridSearchCV(SVC_classifier, parameters)\n",
    "clf.fit(training_review, training_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the best results, you can just use the print function and the optimal combination of hyperparameter values would be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 RandomizedSearchCV\n",
    "\n",
    "The RandomizedSearch method will randomly select the values of hyperparameters. You can feed the distribution to a hyperparameter and see what's going on. Moreover, this method also allows you to specify the number of parameter settings that are sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "parameters2 = {'C':sp_expon(scale=100),'gamma':sp_expon(scale=.1),'kernel':['rbf']}\n",
    "n_iter_search = 20\n",
    "SVC_classifier = SVC()\n",
    "clf2 = RandomizedSearchCV(SVC_classifier, param_distributions=parameters2, n_iter=n_iter_search)\n",
    "clf2.fit(training_review, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=183.58459329130903, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.035711265327240588,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf2.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Bayesian Optimization\n",
    "\n",
    "In machine learning, we always need to find the best $x^{*}$ which satisfies the following:\n",
    "\n",
    "\\begin{equation}\n",
    "x^{*} = \\mathop{\\arg\\max}_{x} f(x)\n",
    "\\end{equation}\n",
    "\n",
    "Bayesian Optimization is extremely great when you are in the following situation:\n",
    "1. f is a black box function with no closed form or gradients\n",
    "2. f is expensive to evaluate\n",
    "3. You may only have noisy observations of f\n",
    "\n",
    "More information about Bayesian Optimization can go to PyData 2017: [PyData 2017](https://pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
