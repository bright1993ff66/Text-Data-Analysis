{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we are going to use multiple classifiers to split the positive and negative reviews. With scikit-learn, it is now very convenient to train the data with much more classifers and much less codes.\n",
    "\n",
    "For the tasks of classification, we can use Naive Bayes, Logistics Regression, etc to train the data. Firstly, I will give a brief introduction about the priciples of these algorithms. Then I will apply these algorithms to the dataset so that we can see which one performs the best. Moreover, different features will be used for trainning. The first featureset is the top 5000 most frequent words while the second featureset is a collection of top 5000 most frequent adj&adv. \n",
    "\n",
    "In the Trainning Part II, I will use multilayer perception to do the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classifiers for Classification\n",
    "\n",
    "In this section, we are going to use Naive Bayes, Multinomial Naive Bayes, Bernoulli Classifier, Logistics Regression Classifier, SGD Classifier, SVC Classifier, Linear SVC Classifier, NuSVC classifier to do the classification. More information about these classifiers can be found here:[Classification - Supervised Learning](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, Naive Bayes actually represents a set of supervied learning algorithms based on applying Bayes' theorem with **the \"naive\" assumption of independence between every pair of features**. Given a class variable $y$ and a dependent feature vector $x_1$ through $x_n$, Bayes theorem states the following relationship:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(y|x_1...x_n) = \\frac{P(y)P(x_1..x_n|y)}{P(x_1...x_n)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above formular, we find calculating $P(x_1...x_n|y)$ is very difficult. But using the \"naive\" assumption, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_{n}) = P(x_i|y) \\quad i = 1...n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then the relationship can be simplified to:\n",
    "\n",
    "$P(y|x_1...x_n) = \\frac{P(y)\\prod_{i=1}^{n}P(x_i|y)}{P(x_1...x_n)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator and $P(y)$ are constants and we could get $y$ by maximizing the $P(y)\\prod_{i=1}^{n}P(x_i|y)$, which means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\ty = \\mathop{\\arg\\max}_{y}P(y)\\prod_{i=1}^{n}P(x_i|y) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier has many versions. The difference between these versions is that they have different methods to approximate $P(x_i|y)$. For the version that we explained above, if we want to use it to do the classification, just use the **NaiveBayesClassifier** in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algo accuracy percent:  68.0\n",
      "Most Informative Features\n",
      "               ludicrous = True              neg : pos    =     10.6 : 1.0\n",
      "               addresses = True              pos : neg    =      9.7 : 1.0\n",
      "                  hudson = True              neg : pos    =      9.6 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.2 : 1.0\n",
      "                  annual = True              pos : neg    =      9.1 : 1.0\n",
      "                   dread = True              pos : neg    =      9.1 : 1.0\n",
      "                   vocal = True              pos : neg    =      9.1 : 1.0\n",
      "                    scum = True              pos : neg    =      8.4 : 1.0\n",
      "                  feeble = True              neg : pos    =      7.0 : 1.0\n",
      "                predator = True              neg : pos    =      7.0 : 1.0\n",
      "               strongest = True              pos : neg    =      6.6 : 1.0\n",
      "               hawthorne = True              pos : neg    =      6.4 : 1.0\n",
      "                 cunning = True              pos : neg    =      6.4 : 1.0\n",
      "                  suvari = True              neg : pos    =      6.3 : 1.0\n",
      "                  symbol = True              pos : neg    =      6.2 : 1.0\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print('Naive Bayes Algo accuracy percent: ', (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another version: Gausian Naive Bayes implements Gaussian Naive Bayes algorithm for classification. This method approximate $P(x_i|y)$ by:\n",
    "\n",
    "$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi{\\sigma_y}^2}}exp(-\\frac{(x_i-{\\mu_y})^2}{2{\\sigma_y}^2})$\n",
    "\n",
    "The parameters $\\sigma_y$ and $\\mu_y$ are estimated by maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gaussian NB accuracy rate is:  60.0 %\n",
      "Wall time: 612 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "training_review = []\n",
    "training_target = []\n",
    "test_review = []\n",
    "test_target = []\n",
    "\n",
    "for text in training_set:\n",
    "    training_review.append(list(text[0].values()))\n",
    "    training_target.append(text[1])\n",
    "    \n",
    "for text in testing_set:\n",
    "    test_review.append(list(text[0].values()))\n",
    "    test_target.append(text[1])\n",
    "    \n",
    "predictions = gnb.fit(training_review, training_target).predict(test_review)\n",
    "count = 0\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == test_target[i]:\n",
    "        count += 1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "print('The Gaussian NB accuracy rate is: ', (count/len(predictions))*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the Multinomial Naive Bayes estimate the $P(x_i|y)$ by the following way: \n",
    "\n",
    "$\\hat{\\theta_{yi}} = \\frac{N_{yi} + \\alpha}{N_{y} + \\alpha n}$\n",
    "\n",
    "where $N_{yi}$ means the number of times feature i appears in a sample of class y in the training set. $N_y$ means the total number of all features for class y. For this movie review data set, we can also use multinomial NB classifier to train the data and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy percent:  70.0\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)                           \n",
    "print('MNB_classifier accuracy percent: ', (nltk.classify.accuracy(MNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last kind of Naive Bayes approach we are going to introduce is the Bernoulli Naive Bayes classifer, which is based on multivariate bernoulli distribution. There might be multiple features but each one is assumed to be **a binary-valued variable**. For this classifier, it estimates $P(x_i|y)$ by:\n",
    "\n",
    "$P(x_i|y) = P(i|y)x_i + (1-P(i|y))(1-x_i)$\n",
    "\n",
    "i means feature. So the implementation of this method on movie review corpora is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli_classifier accuracy percent:  70.0\n"
     ]
    }
   ],
   "source": [
    "Bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "Bernoulli_classifier.train(training_set)                           \n",
    "print('Bernoulli_classifier accuracy percent: ', (nltk.classify.accuracy(Bernoulli_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
