{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we are going to use multiple classifiers to split the positive and negative reviews. With scikit-learn, it is now very convenient to train the data with much more classifers and much less codes.\n",
    "\n",
    "For the classification tasks, we can use Naive Bayes, Logistics Regression, etc to train the data. Firstly, I will give a brief introduction about the priciples of these algorithms. Then I will apply these algorithms to the dataset so that we can see which one performs the best. Moreover, for the type of features, I am going to use the most frequent 5000 words as featuresets. If you chose adjectives and adverbs, the accuracy rate would be higher. You could just use the codes below but don't forget to change the featuresets. \n",
    "\n",
    "In the Trainning Part II, I will use multilayer perception to do the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classifiers for Classification\n",
    "\n",
    "In this section, we are going to use Naive Bayes, Multinomial Naive Bayes, Bernoulli Classifier, Logistics Regression Classifier, SGD Classifier, SVC Classifier, Linear SVC Classifier, NuSVC classifier to do the classification. More information about these classifiers can be found here:[Classification - Supervised Learning](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, Naive Bayes actually represents a set of supervied learning algorithms based on applying Bayes' theorem with **the \"naive\" assumption of independence between every pair of features**. Given a class variable $y$ and a dependent feature vector $x_1$ through $x_n$, Bayes theorem states the following relationship:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    P(y|x_1...x_n) = \\frac{P(y)P(x_1..x_n|y)}{P(x_1...x_n)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above formular, we find calculating $P(x_1...x_n|y)$ is very difficult. But using the \"naive\" assumption, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_{n}) = P(x_i|y) \\quad i = 1...n\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then the relationship can be simplified to:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(y|x_1...x_n) = \\frac{P(y)\\prod_{i=1}^{n}P(x_i|y)}{P(x_1...x_n)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator and $P(y)$ are constants and we could get $y$ by maximizing the $\\prod_{i=1}^{n}P(x_i|y)$, which means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\ty = \\mathop{\\arg\\max}_{y}P(y)\\prod_{i=1}^{n}P(x_i|y) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier has many versions. The difference between these versions is that they have different methods to approximate $P(x_i|y)$. For the version that we explained above, if we want to use it to do the classification, just use the **NaiveBayesClassifier** in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algo accuracy percent:  75.0\n",
      "Most Informative Features\n",
      "                  avoids = True              pos : neg    =     12.5 : 1.0\n",
      "              astounding = True              pos : neg    =     11.9 : 1.0\n",
      "             fascination = True              pos : neg    =     10.5 : 1.0\n",
      "                fairness = True              neg : pos    =      8.8 : 1.0\n",
      "                predator = True              neg : pos    =      8.2 : 1.0\n",
      "                  feeble = True              neg : pos    =      8.2 : 1.0\n",
      "             overwrought = True              neg : pos    =      6.9 : 1.0\n",
      "            breathtaking = True              pos : neg    =      6.7 : 1.0\n",
      "                 insipid = True              neg : pos    =      6.5 : 1.0\n",
      "                    noah = True              pos : neg    =      6.4 : 1.0\n",
      "               hawthorne = True              pos : neg    =      6.4 : 1.0\n",
      "                supports = True              pos : neg    =      6.4 : 1.0\n",
      "                    mena = True              neg : pos    =      6.2 : 1.0\n",
      "                  purple = True              pos : neg    =      5.8 : 1.0\n",
      "                  polley = True              pos : neg    =      5.8 : 1.0\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print('Naive Bayes Algo accuracy percent: ', (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another version: Gausian Naive Bayes implements Gaussian Naive Bayes algorithm for classification. This method approximate $P(x_i|y)$ by:\n",
    "\n",
    "\\begin{equation}\n",
    "P(x_i|y) = \\frac{1}{\\sqrt{2\\pi{\\sigma_y}^2}}exp(-\\frac{(x_i-{\\mu_y})^2}{2{\\sigma_y}^2})\n",
    "\\end{equation}\n",
    "\n",
    "The parameters $\\sigma_y$ and $\\mu_y$ are estimated by maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gaussian NB accuracy rate is:  51.0 %\n",
      "Wall time: 593 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "training_review = []\n",
    "training_target = []\n",
    "test_review = []\n",
    "test_target = []\n",
    "\n",
    "for text in training_set:\n",
    "    training_review.append(list(text[0].values()))\n",
    "    training_target.append(text[1])\n",
    "    \n",
    "for text in testing_set:\n",
    "    test_review.append(list(text[0].values()))\n",
    "    test_target.append(text[1])\n",
    "    \n",
    "predictions = gnb.fit(training_review, training_target).predict(test_review)\n",
    "count = 0\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == test_target[i]:\n",
    "        count += 1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "print('The Gaussian NB accuracy rate is: ', (count/len(predictions))*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the Multinomial Naive Bayes estimate the $P(x_i|y)$ by the following way: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\theta_{yi}} = \\frac{N_{yi} + \\alpha}{N_{y} + \\alpha n}\n",
    "\\end{equation}\n",
    "\n",
    "where $N_{yi}$ means the number of times feature i appears in a sample of class y in the training set. $N_y$ means the total number of all features for class y. For this movie review data set, we can also use multinomial NB classifier to train the data and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy percent:  76.0\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)                           \n",
    "print('MNB_classifier accuracy percent: ', (nltk.classify.accuracy(MNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last kind of Naive Bayes approach we are going to introduce is the Bernoulli Naive Bayes classifer, which is based on multivariate bernoulli distribution. There might be multiple features but each one is assumed to be **a binary-valued variable**. For this classifier, it estimates $P(x_i|y)$ by:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(x_i|y) = P(i|y)x_i + (1-P(i|y))(1-x_i)\n",
    "\\end{equation}\n",
    "\n",
    "i means feature. So the implementation of this method on movie review corpora is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli_classifier accuracy percent:  74.0\n"
     ]
    }
   ],
   "source": [
    "Bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "Bernoulli_classifier.train(training_set)                           \n",
    "print('Bernoulli_classifier accuracy percent: ', (nltk.classify.accuracy(Bernoulli_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Logistics Regression\n",
    "\n",
    "Logistics regression, also known as logit regression, maximum-entropy classification(MaxEnt) or the log-linear classifier, is a linear model for classification. In this model, the probabilities describing the possible outcomes of a single trial are modeled using the following logistic function:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = \\frac{L}{1 + e^{-k(x-x_0)}}\n",
    "\\end{equation}\n",
    "\n",
    "where L means the curve's maximum value, $x_0$ means the x-value of the sigmoid's midpoint, and k means the stepness of the curve. More information about this function can be found here: [logistic function](https://en.wikipedia.org/wiki/Logistic_function)\n",
    "\n",
    "In general, we always use the following form of logistic function:\n",
    "\n",
    "\\begin{equation}\n",
    "    h_{\\theta}(x) = \\frac{L}{1 + e^{-\\theta x}}\n",
    "\\end{equation}\n",
    "\n",
    "The above function is also called sigmoid function, which is also widely used as activation function in neural network. \n",
    "\n",
    "The cost function for logistics regression is:\n",
    "\n",
    "\\begin{equation}\n",
    "    J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}[y_i logh_{\\theta}(x) + (1 - y_i)log(1-h_{\\theta}(x))]\n",
    "\\end{equation}\n",
    "\n",
    "Why do we choose this cost function? How to get this cost function?\n",
    "\n",
    "Firstly, because if we just calculat the estimation $h_{\\theta}(x)$ and plug it in the ordinary cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "    J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - h_{\\theta}(x))\n",
    "\\end{equation}\n",
    "\n",
    "$J(\\theta)$ is non-convex and we can't find the global minimal. Hence we decide to change the cost function. For the cost function except the $\\frac{1}{m}$ and summation, we could use the following trick to deal with predictions:\n",
    "\n",
    "\\begin{equation}\n",
    "cost=\n",
    "\\begin{cases}\n",
    "-log(h_{\\theta}(x))& \\text{y=1}\\\\\n",
    "-log(1-h_{\\theta}(x))& \\text{y=0}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Since y is a variable with only two possible values(y=1 and y=0), then the above thing actually equal to the cost function of logistics regression we have listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In scikit-learn, we can use the LogisticRegression() to construct the classifier. The classifier we constructed in this way can **fit binary**, **One-vs- Rest**, or **multinomial logistic regression** with **optional L2** or **L1** regularization. For this movie reviews dataset, if we use logistics regression, we can work out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_classifier accuracy percent:  67.0\n"
     ]
    }
   ],
   "source": [
    "logistic_classifier = SklearnClassifier(LogisticRegression())\n",
    "logistic_classifier.train(training_set)                           \n",
    "print('logistic_classifier accuracy percent: ', (nltk.classify.accuracy(logistic_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 SGD Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier implements linear classifier such as SVM and logistics regression together with stochastic gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD_classifier accuracy percent:  71.0\n"
     ]
    }
   ],
   "source": [
    "SGD_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGD_classifier.train(training_set)                           \n",
    "print('SGD_classifier accuracy percent: ', (nltk.classify.accuracy(SGD_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 SVM\n",
    "\n",
    "SVC Classifer actually means C-support support vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
